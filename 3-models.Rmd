---
title: "chemo-dd"
subtitle: "Models"
output: 
  html_notebook:
    toc: true
    toc_depth: 4
author: "James Orr"
---

This notebook uses the processed data from `1-organation.Rmd` for modelling. We use various approaches to quantify the curvature of the density-dependence.

### Set up environment 

Load packages and clear environment 

```{r}
#### Required packages
library(tidyverse)  
library(brms)

#### Clear  environment 
rm(list = ls())   
```

Load data

```{r}
all_samples <- read.csv("data/processed/all_samples.csv")
equilibrium_samples <- read.csv("data/processed/equilibrium_samples.csv")
chibio <- read.csv("data/processed/chibio.csv")
```


### Flipped theta-logistic model 

For this first attempt I'll use OD and dilution rate from `equilibrium_samples` - 31 observations, no pseudoreplication. `experiment_replicate` may be a grouping factor that I can use for multi-level models. We manipulated dilution rate (we can be confident that there is no/low error around these values) and OD is our response variable. This means we are kind of flipping the theta-logistic model on its head. 

The standard theta-logistic model is: 

$$
\frac{1}{N} \frac{d N}{d t}=r\left(1-\left(\frac{N}{K}\right)^\theta\right)
$$
Where $\frac{1}{N} \frac{d N}{d t} = g$ is the per capita growth (i.e. `dilution_rate`), $N$ is the abundance/density (i.e., `avg_od_blanked`), $r$ is the intrinsic growth rate, $K$ is the carrying capacity, and $\theta$ is the parameter that controls the shape of density dependence. 
When $\theta$ is 1, it is linear (i.e., logistic growth model), when it is >1 the density dependence is concave (what we expect based on consumer-resource theory), when it is <1 the density dependence is sublinear (a hot topic at the moment). 

We will be using `brms` to estimate the values of $r$, $K$, and most importantly $\theta$, based on the relationship between $g$ and $N$. However, as we manipulated per capita growth, we need to express the theta-logistic model in terms of $N$ (our response variable given our experimental design). 

Starting from

$$
g=r\left(1-\left(\frac{N}{K}\right)^\theta\right)
$$
we can rearrange to

$$
N=K\left(1-\frac{g}{r}\right)^{\frac{1}{\theta}}
$$


#### first attempt with brms

Will use Gamma error distribution with the identity link - accounts for the fact that we can't have negative OD (may be relevant for the higher dilution rates, where r is trying to be fit). 

Also worth trying to include the random effect of experiment replicate on one/all of the parameters we're trying to estimate. 

Non-linear models in `brms` require fairly informative priors. Make sure prior for theta is very loose as this is the main one we're interested in. 

```{r, include = F}

model <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(avg_od_blanked ~ K * (1 - (dilution_rate / r)) ^ (1 / theta),
     K ~ 1, # + (1|experiment_replicate),
     r ~ 1, #+ (1|experiment_replicate),
     theta ~ 1, # + (1|experiment_replicate),
     nl = TRUE
  ),
  
  # data to use
  data = equilibrium_samples,
  
  # errors are gamma with identity link function (OD can't be negative)
  family = Gamma(link = "identity"), 
  
  # priors 
  prior = c(
    prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
    prior(uniform(0.10, 0.25), nlpar = "K", lb = 0.10, ub = 0.25), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5)  
  ),
  
  # hyperparameters 
  iter = 3000, warmup = 1500, chains = 4,
  control = list(adapt_delta = 0.9,
                 max_treedepth = 10),
  
)
```

```{r}
summary(model)
plot(model)
conditional_effects(model)
pairs(model)
pp_check(model, type = "scatter_avg")
```
With a normal distribution we got a huge amount of divergence and strong boundary effects for the r. In the equation, r is never able to be lower than the maximum of `dilution_rate`, the model becomes undefined (being raised to a fractional power). In theory this is makes sense - r is the maximum possible growth rate, so it should be larger than all observed growth rates. However, there is measurement error associated with `dilution_rate` so that could cause an observed value to be above the best fitting r. 

Using a gamma error distribution improves the divergence issues substantially and gives the posterior distribution of r a much more normal distribution (although bounding issue is still there).

Random effects are really difficult to fit. Even just for one parameter (r), the models don't converge well. I guess this is unsurprising given that the different experiment replicates had different ranges they tested dilution rates over. So we can get very weird estimates for the intercepts when we have no/few values towards those ends of the curves.

The theta-logistic model just doesn't seem to be flexible enough to fit the data extremely well. However, it is clear that theta is significantly above 1 - so we have concave density-dependence, or supralinear growth. 


```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Theta-logistic with user-defined functions
K <- 0.18  
r <- 1.07 
theta <- 2.41  
dilution_rate_seq <- seq(0, 1.1, by = 0.001)
avg_od_blanked_curve <- K * (1 - (dilution_rate_seq / r)) ^ (1 / theta)
curve_data <- data.frame(dilution_rate = dilution_rate_seq, avg_od_blanked = avg_od_blanked_curve)

# Create the plot with the original data points and model predictions
ggplot(equilibrium_samples, aes(y = avg_od_blanked, x = dilution_rate)) +
  geom_point() +
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  # user defined model
  #geom_line(data = curve_data, aes(y = avg_od_blanked, x = dilution_rate), 
  #            color = "blue", size = 1) +
   
  theme_minimal()
```
Black line is the model with the gamma errors Blue line is the the model fitted with gaussian errors. 

Flip the axes (for density-dependence perspective) and fit a theta-logistic model with parameters from above. 

```{r}

# Theta-logistic model with parameters from above
K <- 0.20   
r <- 1.08   
theta <- 1.84  
od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- r * (1 - (od_seq / K)^ theta)
curve_data <- data.frame(avg_od_blanked = od_seq, 
                         dilution_rate = dilution_curve)
curve_data <- curve_data %>% filter(dilution_rate > 0)


# Density-dependent plot
ggplot(equilibrium_samples, aes(x = avg_od_blanked, y = dilution_rate)) +
  geom_point() +
  labs(x = "OD", y = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # theta-logistic
  geom_line(data = curve_data, aes(x = avg_od_blanked, y = dilution_rate), 
              color = "blue", size = 1) +
  
  theme_minimal()
```

**problem when $\frac{g}{r} > 1$**

What is causing this affect?

If $r = g$, then 0 will be raised to the power of $1/\theta$ which will just give 0, so I can see why the model would struggle there. 

If $r < g$, then a negative value will be raised to the power of $1/\theta$ which also seems to be problematic. Raising a negative value to a fractional power seems problematic upon a quick google (complex numbers issues, undefined issues)

In theory it shouldn't be possible for us to get r values that are smaller than g, as g occurs at the intercept. However, our model is presumably not following theta-logistic perfectly and there is also error associated with `dilution_rate`. Not immedietely clear what to do from here. 

It seems that "reparameterization" is needed - one option is to transform r. I've tried using exp(log_r) and exp(r) and inv_r - but none of these made a difference. I think the issues are with the raising to the power of a fraction? 

#### finding a better parameterization for r

I've tried using exp(log_r), exp(r), inv_v, inv_theta - none of these seemed to make a difference

I also tried a logarithmic transformation: 

$$
\ln (N)=\ln (K)+\frac{1}{\theta} \ln \left(1-\frac{g}{r}\right)
$$

```{r, include = F}

model_trans <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(
    log(avg_od_blanked) ~ log(K) + (1 / theta) * log( 1 - (dilution_rate / r)),
    #avg_od_blanked ~ K * (1 - (dilution_rate / r)) ^ (1 / theta),
    #avg_od_blanked ~ K * (1 - (((dilution_rate + e)) / r)) ^ (1 / theta),
    #avg_od_blanked ~ K * (1 - (dilution_rate / r)) ^ invtheta,
    #avg_od_blanked ~ K * (1 - (dilution_rate / r)) ^ exp(loginvtheta),
    K ~ 1,  
    r ~ 1,
    theta ~ 1,
    #invtheta ~ 1,
    #e ~ 1,
    #loginvtheta ~ 1,
    nl = TRUE  
  ),
  
  # data to use
  data = equilibrium_samples,
  
  # errors are gaussian
  family = gaussian(), 
  
  # priors 
  prior = c(
    prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
    prior(uniform(0.1, 0.25), nlpar = "K", lb = 0.1, ub = 0.25), 
    prior(uniform(0.5, 5), nlpar = "theta", lb = 0.5, ub = 5)
    #prior(uniform(log(0.2), log(10)), nlpar = "loginvtheta", lb = log(0.2), ub = log(10))
    #prior(uniform(-1, 1), nlpar = "e", lb = -1, ub = 1)  
  ),
  
  # hyperparameters 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.97,
                 max_treedepth = 10),
  
)
```


```{r}
summary(model_trans)
plot(model_trans)
conditional_effects(model_trans)
pairs(model_trans)
pp_check(model_trans, type = "scatter_avg")
```

```{r}
conditional_effects_data <- conditional_effects(model_trans)

predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

ggplot(equilibrium_samples, aes(y = log(avg_od_blanked), x = dilution_rate)) +
  geom_point() +
  
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  theme_minimal()
```

This has the same issue as before (undefined when r is smaller than g). However, I guess because of the transformation the fit of this model is a lot better, there are basically no divergences, but r is still bounded - with a reasonably tight prior for r I get slightly better results, but interpreting all of this seems odd. Does theta still mean the density dependence that we thought? Probably not.. 

Also - should the error distribution be changed to Gamma as well? Probably doesn't need to be?


#### Measurement error


I had the idea of adding an "error" term to dilution rate - this would allow r to be smaller than g (if e was negative enough) - really not sure if this makes any sense.

$$
N=K\left(1-\frac{g + e}{r}\right)^{\frac{1}{\theta}}
$$


There is a `brms` function called `mi()` that can be used to do this probably. Need to investigate that!

https://discourse.mc-stan.org/t/measurement-error-in-nonlinear-mixed-effect-models/17610/5

mi/me don't work directly with non-linear functions. You have to set up latent variables for them. Based on the link above (question answered by package author) if you do `X ~ 1 + me(dilution_rate, sd)` that would also include an intercept, which he thinks you don't need? He suggested doing: `X ~ 0 + me(dilution_rate, sd)`. 

```{r, include = F}
model_me <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(avg_od_blanked ~ K * (1 - (X / r)) ^ (1 / theta),
     X ~ 0 + me(dilution_rate, 0.005),
     K ~ 1, # + (1|experiment_replicate),
     r ~ 1, #+ (1|experiment_replicate),
     theta ~ 1, # + (1|experiment_replicate),
     nl = TRUE
  ),
  
  # data to use
  data = equilibrium_samples,
  
  # errors are gamma with identity link function (OD can't be negative)
  family = Gamma(link = "identity"), 
  
  # priors 
  prior = c(
    prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
    prior(uniform(0.10, 0.25), nlpar = "K", lb = 0.10, ub = 0.25), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5)  
  ),
  
  # hyperparameters 
  iter = 3000, warmup = 1500, chains = 4,
  control = list(adapt_delta = 0.9,
                 max_treedepth = 10),
  
)
```

```{r}
summary(model_me)
plot(model_me)
conditional_effects(model_me)
pairs(model_me)
pp_check(model_me, type = "scatter_avg")
```


```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(model_me)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)


# Create the plot with the original data points and model predictions
ggplot(equilibrium_samples, aes(y = avg_od_blanked, x = dilution_rate)) +
  geom_point() +
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  # user defined model
  #geom_line(data = curve_data, aes(y = avg_od_blanked, x = dilution_rate), 
  #            color = "blue", size = 1) +
   
  theme_minimal()
```




### Polynomial models 

Run a linear model, quadratic model, and quadratic model with random effects

```{r, include=FALSE}

########## linear model ##########
linear_model <- brm(
  formula = avg_od_blanked ~ dilution_rate,
  data = equilibrium_samples,
  family = gaussian(), 
  priors <- c(
    set_prior("normal(0, 10)", class = "Intercept"),           
    set_prior("normal(0, 5)", class = "b", coef = "dilution_rate")   
    ), 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10)
  )

########## quadratic model ##########
quad_model <- brm(
  formula = avg_od_blanked ~ dilution_rate + I(dilution_rate^2), 
  data = equilibrium_samples,
  family = gaussian(), 
  priors <- c(
    set_prior("normal(0, 10)", class = "Intercept"),           
    set_prior("normal(0, 5)", class = "b", coef = "dilution_rate"),   
    set_prior("normal(0, 5)", class = "b", coef = "Idilution_rateE2")
    ), 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10)
  )

########## quadratic model with random effects ##########
quad_model_random <- brm(
  formula = avg_od_blanked ~ dilution_rate + I(dilution_rate^2) +
    (1 + dilution_rate + I(dilution_rate^2) | experiment_replicate),  
  data = equilibrium_samples,
  family = gaussian(), 
  priors <- c(
    set_prior("normal(0, 10)", class = "Intercept"),           
    set_prior("normal(0, 5)", class = "b", coef = "dilution_rate"),   
    set_prior("normal(0, 5)", class = "b", coef = "Idilution_rateE2"), 
    set_prior("normal(0, 2)", class = "sd", group = "experiment_replicate"), 
    set_prior("normal(0, 2)", class = "sd", group = "experiment_replicate", coef = "dilution_rate"), 
    set_prior("normal(0, 2)", class = "sd", group = "experiment_replicate", coef = "Idilution_rateE2"),
    set_prior("normal(0, 2)", class = "sigma")
    ), 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10)
  )

########## cubic model ##########
cubic_model <- brm(
  formula = avg_od_blanked ~ dilution_rate + I(dilution_rate^2) + I(dilution_rate^3) , 
  data = equilibrium_samples,
  family = gaussian(), 
  priors <- c(
    set_prior("normal(0, 10)", class = "Intercept"),           
    set_prior("normal(0, 5)", class = "b", coef = "dilution_rate"),   
    set_prior("normal(0, 5)", class = "b", coef = "Idilution_rateE2"),
    set_prior("normal(0, 5)", class = "b", coef = "Idilution_rateE3")
    ), 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10)
  )
```

```{r}
pp_check(linear_model, ndraws = 20)
pp_check(quad_model, ndraws = 20)
pp_check(quad_model_random, ndraws = 20)
pp_check(cubic_model, ndraws = 20) 
```


```{r}
loo(linear_model, quad_model, quad_model_random, cubic_model)
```
Seems like quadratic without random effects is best. Cubic has very similar fit, so no benefit of including it. 


```{r}
summary(quad_model)
plot(quad_model)
conditional_effects(quad_model)
```

Intercept term controls the height of the curve (where the turning point is on the y axis)

Linear term (dilution_rate) controls the position of the curve (shifting to the centre of mass of the curve around the place)

The quadratic term (Idilution_rateE2) controls the width of the curve

```{r}
conditional_effects_data <- conditional_effects(quad_model)

predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)


ggplot(equilibrium_samples, aes(y = avg_od_blanked, x = dilution_rate)) +
  geom_point() +
  
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)") +
  theme_minimal()
```




### Power model

We can also try a power model. We are looking for a simple power model that is very flexible but that is decreasing and that can be linear, concave, or convex. A simple one that we played around with in desmos was:

$$ y = ax^\frac{1}{\beta} + c $$
Where $y$ is equal to OD, $x$ is equal to dilution rate, $a$ is related to where the curve cross the $x$ axis (but not quite intercept as it varies depending on $\beta$ - it must be negative for the function to be decreasing), $\beta$ is the curvature (linear when 1, convex when greater than 1, and concave when lower than 1, must be above 0), and $c$ is the y intercept (when x is 0). 

```{r, include = F}

power_model <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(avg_od_blanked ~ a * (dilution_rate^(1/b)) + c, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = equilibrium_samples,
  
  # Errors are gamma with identity link function
  family = Gamma(link = "identity"),
  
  # Priors
  prior = c(
    prior(uniform(-5, 5), nlpar = "a", lb = -5, ub = 5),
    prior(uniform(0, 2), nlpar = "b", lb = 0, ub = 2),
    prior(uniform(0, 2), nlpar = "c", lb = 0, ub = 2)
  ),
  
  # Hyperparameters
  iter = 3000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.9, max_treedepth = 10)
)

```

```{r}
summary(power_model)
plot(power_model)
conditional_effects(power_model)
pairs(power_model)
pp_check(power_model, type = "scatter_avg")
```

```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(power_model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Create the plot with the original data points and model predictions
ggplot(equilibrium_samples, aes(y = avg_od_blanked, x = dilution_rate)) +
  geom_point() +
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
   
  theme_minimal()
```

Using gamma or Gaussian errors doesn't change the general fit or parameter estimates. Gamma probably makes sense from a more general theoretical perspective? 




#### And with colony counts

`avg_cfu_ul` is not an integer as we sometimes take the average of multiple days. So can't use poisson - unless I round? 

It is also on a very different scale so need to update priors for c (y axis intercept) and may need to rescale so that the parameters are roughly same scale - to help with fitting 

```{r, include = F}

equilibrium_samples$avg_cfu_rescale <- round(equilibrium_samples$avg_cfu_ul/10000)

power_model_count <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(avg_cfu_rescale ~ a * (dilution_rate^(1/b)) + c, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = equilibrium_samples,
  
  # Errors are poisson
  family = poisson,
  #family = Gamma(link = "identity"),
  
  # Priors
  prior = c(
    prior(uniform(-5, 5), nlpar = "a", lb = -5, ub = 5),
    prior(uniform(0, 2), nlpar = "b", lb = 0, ub = 2),
    prior(uniform(0, 150), nlpar = "c", lb = 0, ub = 150)
  ),
  
  # Hyperparameters
  iter = 3000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.9, max_treedepth = 10)
)

```

```{r}
summary(power_model_count)
plot(power_model_count)
conditional_effects(power_model_count)
pairs(power_model_count)
pp_check(power_model_count, type = "scatter_avg")
```

```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(power_model_count)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Create the plot with the original data points and model predictions
ggplot(equilibrium_samples, aes(y = avg_cfu_rescale, x = dilution_rate)) +
  geom_point() +
  labs(y = "CFU", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
   
  theme_minimal()
```





### Theta-logistic on flipped axes

I should probably disregard this as regression is asymmetric, in the sense that it really matters what is the response and what is the predictor in terms of how variance is explained. It is assumed that the predicts are known with certainty but that there is uncertainty (residuals) around the response variable. Our experiment has OD as the response variable and dilution_rate as the predictor variable - so we have to model them that way around. No point looking at the proper theta-logistic model. 

```{r, include = F}

model2 <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(
    dilution_rate ~ r * (1 - (avg_od_blanked / K)^ theta),
    K ~ 1,  
    r ~ 1,
    theta ~ 1, 
    nl = TRUE  
  ),
  
  # data to use
  data = equilibrium_samples,
  
  # errors are gaussian
  family = gaussian(), 
  
  # priors 
  prior = c(
    prior(uniform(0.8, 1.3), nlpar = "r", lb = 0.8, ub = 1.3),  
    prior(uniform(0.15, 0.25), nlpar = "K", lb = 0.15, ub = 0.25), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5)  
  ),
  
  # hyperparameters 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10),
  
  # inital values to help with start up
  #init = function() list(K = 0.20, r = 1.1)
)
```

```{r}
summary(model2)
plot(model2)
conditional_effects(model2)
pairs(model2)
pp_check(model2, ndraws = 20)
```


```{r}
# Generate conditional effects from the model
conditional_effects_data <- conditional_effects(model2)

# Extract the data for the relevant variable, 
predicted_data <- as.data.frame(conditional_effects_data$avg_od_blanked)

# Theta-logistic model with user-defined parameters
K <- 0.18   
r <- 1.03   
theta <- 2.41  
od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- r * (1 - (od_seq / K)^ theta)
curve_data <- data.frame(avg_od_blanked = od_seq, 
                         dilution_rate = dilution_curve)
curve_data <- curve_data %>% filter(dilution_rate > 0)

# Create the plot with the original data points and model predictions
ggplot(equilibrium_samples, aes(x = avg_od_blanked, y = dilution_rate)) +
  geom_point() +
  labs(x = "OD", y = "Dilution rate (ml/hour/ml)", 
      color = "experiment") +
  
  # model predictions  
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = avg_od_blanked)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = avg_od_blanked), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  # user-defined theta-logistic
  geom_line(data = curve_data, aes(x = avg_od_blanked, y = dilution_rate), 
              color = "blue", size = 1) +
    
  theme_minimal()
```

There are no divergence issues. The actual modelling fitting / parameterization all seems good, but it just seems that theta-logistic isn't the ideal fit for the data. Not quite flexible enough. The Posterior Predictive Checks don't look great. Strong pattern in the residules?

max OD is: 0.18980, posteriors for K are able to cross this - so that doesn't seem to be an issue here. 

Don't know how much we should trust this as here we are assuming there is no error in OD and that there is error in dilution_rate - this is the opposite to what we think. So I don't think fitting these models this way really makes sense. 

