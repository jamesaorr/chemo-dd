---
title: "chemo-dd"
subtitle: "Main analysis"
output: 
  html_notebook:
    toc: true
    toc_depth: 3
author: "James Orr"
---

This notebook uses the processed data from `1-data-prep.Rmd` and the estimated Monod parameters for the E. coli strain used in our study (in the fits sub folder) to make the predictions for density dependence. 


## Set up environment 

Load packages and clear environment 

```{r}
#### Required packages
library(tidyverse)                 # general data organisation
library(deSolve)                   # solving differential equations
library(cowplot)                   # arranging plots 
library(posterior)                 # to extract posterior samples 
library(brms)                      # bayesian regression models using stan 
library(HDInterval)                # for HDI credible intervals 

#### Clear  environment 
rm(list = ls())   

#### colours
j_blue <- rgb(0, 112/255, 192/255)
j_green <- rgb(87/255, 167/255, 46/255)


#### Set seed for reproducibility
set.seed(22)
```

## Theory

The model we'll use is a consumer-resource model for one resource (glucose) and one consumer (E. coli) where the resource uptake function is a Monod function and the resource supply function is a chemostat model. From previous work with this system, we know that this simple model does a very good job of describing the dynamics of E. coli and glucose. 


$$
\frac{dN}{dt}  = N( \frac{\mu_{max}R}{k + R} -m) \\
\frac{dR}{dt} = d(S - R) - \frac{\mu_{max}R}{k + R}QN
$$
$N$ = density of consumers

$R$ = density of resources

$\mu_{max}$ = maximum growth rate 

$k$ = half saturation constant

$m$ = mortality rate 

$d$ = dilution rate

$S$ = density of resources in supply

$Q$ = resource quota (how many resources are in one consumer)


Define the consumer-resource functions that we'll need 

```{r}
# Type II functional response and chemostat resources
cr_tII_chemo <- function(t, state, params) {
  with(as.list(c(state, params)), {
    dN_dt <- N * ( (mu_max * R / (k + R)) - m)
    dR_dt <- (d * (S - R)) - ( (mu_max * R / (k + R)) * Q * N )
    list(c(dN_dt, dR_dt))
  })
}

# Type II functional response and logistic resources
cr_tII_log <- function(t, state, params) {
  with(as.list(c(state, params)), {
    dN_dt <- N * ( (mu_max * R / (k + R)) - m)
    dR_dt <- (r*R*(1 - R/K)) - ( (mu_max * R / (k + R)) * Q * N )
    list(c(dN_dt, dR_dt))
  })
}

# Type II functional response with no resource growth (events will be used to add pulses)
cr_tII_pulse <- function(t, state, params) {
  with(as.list(c(state, params)), {
    dN_dt <- N * ((mu_max * R / (k + R)) - m)
    dR_dt <- - ((mu_max * R / (k + R)) * Q * N)
    list(c(dN_dt, dR_dt))
  })
}

# Define pulse event function: adds resource at specific times
pulse_fun <- function(t, state, parms) {
  with(as.list(c(state, parms)), {
    R <- R + pulse_size
    return(c(N, R))
  })
}

```




#### Quantifying density dependence

We used Abrams’ approach – manipulating mortality rate and observing the resulting population densities at equilibrium – to study the density dependence of Escherichia coli (MG1655) growing in chemostats. However, in our system, dilution rate of the chemostat is equal to the mortality rate of the bacteria, as we assume that there is no additional consumer mortality. We therefore experimentally manipulated the dilution rate of the chemostats to control the mortality of the consumer. This deviation from Abrams’ precise approach – dilution rate is not a “neutral” parameter in the sense that it alters resource dynamics as well as consumer mortality – was necessary in our system, but it allowed us to isolate the qualitative effect of the consumers’ functional responses on their own density dependence. 

We can interpret the empirical observations as a form of density dependence (independent of resource dynamics) and we can make predictions for density dependence under resource dynamics that were not experimentally tested. 

We can just simulate the consumer resource model under different dilution rates (where m = d) and plot equilibrium abundances against dilution rates. We can also analytically predict the shape of density dependence that we expect to see when d = m. We set the right hand side of the equations to zero and solve for $N$ and $R$. From the consumer equation:

$$R^* = \frac{mK_s}{\mu - m}$$
From the resource equation:

$$N^* = \frac{dSk}{R^*} + ds - dk - dR^*$$
Substituting $R^*$ into $N^*$, we get:

$$N^* = \frac{d}{\mu Q} \left [\frac{\mu S}{m}  - \frac{mk}{\mu - m} - k \right ]$$

```{r}
# Define the N* function for chemostat resources and type II functional response
N_star_chemo <- function(d, m, u, S, k, q) {
  term1 <- (u * S / m)
  term2 <- (m * k / (u - m))
  return((d / (u * q)) * (term1 - term2 - k))
}
```


## Prediction

We have empirical estimates for $\mu_{max}$, $k$, and $Q$ *(note that we have an estimate for yield, which is the inverse of the quota)*. We know the range of dilution rates we used and we are simulating $N$ and $R$. That just leaves $S$ - the supply resource concentration. We used 0.05% glucose as our base media - so $S$ is just 0.05. The units of resources with our parameterized model are R = 1 is 10 mg/ml of glucose. The empirically estimated parameters are in terms of OD for $N$, so the predictions should in theory align with our results (particularly at low dilution rates that match batch cultures - how the models were parameterised - more closely). 

Extract the mean estimates and 100 random posterior draws to propagate uncertainty. 

```{r}
# read in the model fit
monod_fit <- readRDS(paste0('data/fits/MG1655_MCMC_holling_random.Rdata'))

# extract the posterior draws
# rename variables to match growth rate of yield times Holling-like uptake rate
# convert things to Monod growth rate
# and get rid of the poorly named parameters
monod_draws <-
	monod_fit$draws() %>%
	posterior::rename_variables(
		log_a = log_mu,
		log_h = log_k
	) %>%
	posterior::mutate_variables(
		log_mu_max = log_q - log_h,
		log_Ks = - log_a - log_h,
		log_affinity = log_mu_max - log_Ks
	)


# print out the summery values of the key variables
sumparams <- subset_draws(monod_draws, c('log_mu_max', 'log_Ks', 'log_q')) %>% 
  summary()
#round(exp(sumparams$mean), 6)

mu_mean <- exp(sumparams$mean)[1]
k_mean <- exp(sumparams$mean)[2]
q_mean <- 1/exp(sumparams$mean)[3] # q in this model is actually yield, so we need to convert it to quota (inverse)


posterior_samples <- monod_draws %>%
  
  # from monod_draws, just get the posteriors for the key variables
  posterior::subset_draws(c("log_mu_max", "log_Ks", "log_q")) %>%
  
  # turn the draw object into a standard data frame, with one row per draw
  posterior::as_draws_df() %>%
  
  # backtransform to natural scale and convert yield to quota 
  mutate(
    mu_max = exp(log_mu_max),
    Ks = exp(log_Ks),
    q = 1/exp(log_q)
  )

# choose a subset of posteriors to use for predictions to show uncertainty
set.seed(22)  # for reproducibility
posterior_subset <- posterior_samples %>% 
  slice_sample(n = 100) %>%
  mutate(draw = row_number())  # <-- This ensures `draw` exists

rm(monod_fit, posterior_samples, sumparams, monod_draws)
```

Check that the Monod function looks right and run model for a single dilution rates

```{r, fig.height=3, fig.width=6}
###################################
#### Plot estimated Monod curve ###
###################################

# estimate parameters
mu_max <- mu_mean
k <- k_mean

# Generate Monod data
R <- seq(0, 0.05, length.out = 100) 
mu <- mu_max * R / (k + R)
monod_data <- data.frame(R = R, mu = mu)

# Create Monod curve for each posterior sample
posterior_curves <- posterior_subset %>%
  mutate(draw = row_number()) %>%
  crossing(R = R) %>%
  mutate(mu = mu_max * R / (Ks + R))

# Plot Monod function
p1 <- ggplot() +
  geom_line(data = posterior_curves, aes(x = R, y = mu, group = draw), 
            alpha = 0.025, size = 1, color = "blue") +
  geom_line(data = monod_data, aes(x = R, y = mu), 
            size = 0.5, alpha = 1) +  
  labs(x = "R", y = "Growth Rate") +
  ylim(0, 1) +
  theme_minimal() 



###################################
#### Simulate for one dilution ####
###################################

### set initial conditions and paramaters
t <- seq(0, 10000, 1)
initial_conditions <- c(N = 0.01, R = 0.01)
d <- 0.5          # have to define this first, as m is set to d
params <- list(
  mu_max = mu_mean,  
  k = k_mean,          
  S = 0.05,            
  Q = q_mean,        
  d = d,
  m = d
  )

# Run model 
output <- ode(
    y = initial_conditions,
    times = t,
    func = cr_tII_chemo,
    parms = params
  )

# Convert output to a data frame
output_df <- as.data.frame(output)

# Plot dynamics
p2 <- ggplot(output_df, aes(x = time)) +
  geom_line(aes(y = N, color = "N")) +
  geom_line(aes(y = R, color = "R")) +
  labs(color = "") +
  xlim(0, 50) +
  theme_minimal() 

plot_grid(p1, p2)

rm(p1, p2, params, posterior_curves, d, 
   initial_conditions, k, mu, mu_max, R, t, output, output_df, monod_data)
```

Simulating many different consumer-resource models across a range of dilutions gives the same result as the analytical prediction (next two chunks). So we can just use the analytical prediction moving forward. 

```{r, include = FALSE, echo = FALSE}
### set initial conditions and parameters
t <- seq(0, 10000, 100)        # needs to be long enough to get to equilibrium, resolution doens't matter
initial_conditions <- c(N = 0.01, R = 0.01)

# Fixed parameters
params_fixed <- list(
  mu_max = mu_mean,      
  k = k_mean,         
  S = 0.05,                 
  Q = q_mean
  )

# Create a sequence of dilution rates
dil_values <- seq(0.001, exp(-0.383), 0.001)

# Initialize an empty data frame to store results
results <- data.frame(dil = numeric(), final_N = numeric())

# Loop over dilution rates
for (dil in dil_values) {
  # Update parameters with the current dilution rate
  params <- c(params_fixed, d = dil, m = dil) # m is set to d for our experiment
  
  # Solve the model
  output <- ode(
    y = initial_conditions,
    times = t,
    func = cr_tII_chemo,
    parms = params
  )
  
  # Convert output to a data frame
  output_df <- as.data.frame(output)
  
  # Get the final value of N
  final_N <- tail(output_df$N, 1)
  
  # Save the dilution rate and final_N to results
  results <- rbind(results, data.frame(dil = dil, final_N = final_N))
}

```

```{r, fig.width=4, fig.height=4, include = FALSE, echo = FALSE}
#### Analytical prediction
# Define parameters
u <- mu_mean
k <- k_mean
q <- q_mean
S <- 0.05
# Generate prediction
d <- seq(0, u, by = 0.0001)
m <- d
N_star <- N_star_chemo(d, m, u, S, k, q)
prediction <- data.frame(dilution_rate = d, N_star = N_star)

# Plot results
ggplot() +
  geom_line(data = results, aes(x = dil, y = final_N, colour = "simulation"),
            linewidth = 2) +
  geom_line(data = prediction, aes(x = dilution_rate, y = N_star, colour = "prediction")) +
  scale_colour_manual(values = c("simulation" = "#3BDCB4", "prediction" = "#185266")) +
  labs(
    x = "Dilution Rate",
    y = "Equilibrium Density",
    colour = ""
  ) +
  xlim(0, u) +
  ylim(0, 0.25) +
  theme_minimal()

rm(output, output_df, params, params_fixed, prediction, 
   d, dil, dil_values, final_N, initial_conditions, k, m, N_star, q, S, t, u,
   results)
```


Now that we know analytical prediction matches simulations, we can make analytical prediction that incorporates uncertainty from Monod parameter estimates. Here we are predicting what density dependence should have been observed by our experiment (where consumer mortality and dilution rate are equal)


```{r, fig.width=4, fig.height=4}
# Define parameters
u <- mu_mean
k <- k_mean
q <- q_mean
S <- 0.05

# Generate prediction
d <- seq(0, u+0.05, by = 0.0001)
m <- d
N_star <- N_star_chemo(d, m, u, S, k, q)
prediction <- data.frame(dilution_rate = d, N_star = N_star)

# remove negative densities and dilution rates above maximum growth rate 
prediction <- prediction %>%
  filter(N_star > 0) %>%
  filter(dilution_rate < mu_mean)

# add posteriors for uncertainty 
# expand grid of all draws by all dilution values
# crossing is a wrapper around expand_grid() that de-deuplicates and sorts
prediction_posteriors <- posterior_subset %>%
  select(draw, mu_max, Ks, q) %>%
  crossing(dilution_rate = d) %>%
  mutate(
    m = dilution_rate,
    N_star = N_star_chemo(dilution_rate, m, mu_max, S, Ks, q)
  )

# remove all negative densities
# for each row remove dilution rates where dilution rate is above mu_max 
# draws have different mu_max so will have different max dilution rates
prediction_posteriors <- prediction_posteriors %>%
  filter(N_star > 0) %>%
  filter(dilution_rate < mu_max)

# Plot results
ggplot() +
  geom_line(data = prediction_posteriors, aes(x = dilution_rate, y = N_star, group = draw),
            alpha = 0.05, color = j_green) +
  geom_line(data = prediction, aes(x = dilution_rate, y = N_star),
            color = j_green, linetype = "dashed") +
  labs(
    x = "Dilution Rate",
    y = "Equilibrium Density") +
  theme_minimal()

rm(d, k, m, N_star, q, S, u)

```

Save theoretical prediction.   

```{r}
write.csv(prediction, "data/theory/prediction.csv", row.names = F)
write.csv(prediction_posteriors, "data/theory/prediction_posteriors.csv", row.names = F)
```


### Figure 1B

Compare theoretical predictions to empirical results. First we need to fit a model to the empirical data to quantify the degree of superlinearity. We'll use the inverse of the theta logistic model: 

$$
N=K\left(1-\frac{g}{r}\right)^{\frac{1}{\theta}}
$$

See `3-supplementary-models.Rmd` for details on alternative models and fitting approaches. Many other modelling approaches (e.g., inverse theta logistic in `brms`, power model, polynomials) find superlinearity as well. Here, we'll use a grid based approach that is very transparent and that can overcome MCMC issues with the boundary issue, where the parameter r can't be lower than the observed g values. 


Load data 

```{r}
exp_data <- read.csv("data/processed/equilibrium_samples_av.csv")

# rename variables
exp_data$g <- exp_data$dilution_rate
exp_data$Neq_dat <- exp_data$od_blanked
```

Define the function we are fitting. Note the max function, which ensures that the predicted equilibrium density is never negative. 

```{r}
Neq <- function(K, g, r, theta) {
  return(K * max(0, (1 - g/r))^(1/theta))
}
```

Computing posterior probabilities over parameter grid. 

First, we set up a grid of parameter values to screen:

```{r}
#| eval: false # true to actually run it
grid <- crossing(K = seq(0.15, 0.225, length.out = 50),
                 r = seq(0.7, 1.1, length.out = 50),
                 theta = seq(0.5, 10, length.out = 50),
                 sigma = seq(0.01, 0.05, length.out = 20))

# Choosing range for sigma
# quick check of variability in observed data
#sd(exp_data$od_blanked)
```

The number of values for each parameter is chosen to balance computational efficiency while still obtaining smooth posterior distributions for those parameters.

Next, we add some priors that are fairly broad but still include realistic values:

```{r}
#| eval: false # true to actually run it
grid <- grid |>
  mutate(
    log_prior_K = dunif(K, min = 0.15, max = 0.25, log = TRUE),
    log_prior_r = dunif(r, min = 0.7, max = 1.1, log = TRUE),
    log_prior_theta = dunif(theta, min = 0.5, max = 10, log = TRUE),
    log_prior_sigma = dunif(sigma, min = 0.01, max = 0.1, log = TRUE),
    log_prior = log_prior_K + log_prior_r + log_prior_theta + log_prior_sigma
  )
```

Now we calculate the log-likelihood of the parameter combinations in the grid, given the data:

```{r}
#| eval: false # true to actually run it
grid <- grid |>
  rowwise() |>
  mutate(
    mu = list(sapply(exp_data$g, function(x) Neq(K, x, r, theta))),
    log_liks = list(dnorm(exp_data$Neq_dat, mean = mu, sd = sigma, log = TRUE)),
    log_lik = sum(log_liks)
  ) |>
  ungroup()
```

Finally, we can combine the likelihood and priors to obtain the posterior probabilities:

```{r}
#| eval: false # true to actually run it
grid <- grid |>
  mutate(log_posterior = log_lik + log_prior)

max_log_post <- max(grid$log_posterior)
log_sum_exp_post  <- log(sum(exp(grid$log_posterior - max_log_post))) + max_log_post

max_log_prior <- max(grid$log_prior)
log_sum_exp_prior  <- log(sum(exp(grid$log_prior - max_log_prior))) + max_log_prior

grid <- grid |>
  mutate(posterior = exp(log_posterior - log_sum_exp_post)) |>
  mutate(prior = exp(log_prior - log_sum_exp_prior))

rm(max_log_post, log_sum_exp_post, max_log_prior, log_sum_exp_prior)

# select only the columns we need (this really speeds things up down the line)
#grid <- grid %>%
#  select(K, r, theta, sigma, prior, posterior)

#write.csv(grid, "data/grid/grid.csv", row.names = FALSE)
```
The grid above (50 x 50 x 50 x 20) has 2.5 million combinations of parameters, so it takes quite a while to run (~10 mins on an M1 MacBook Pro), so we can just load the grid from a previous run instead:

```{r}
grid <- read.csv("data/grid/grid.csv")
```


To visualise these results, we can first plot the posterior probabilities for each parameter. In these plots, black lines show the posterior probabilities, blue lines the prior probabilities:

```{r, fig.height=2, fig.width=8}
p1 <- grid |>
  group_by(K) |>
  filter(!is.nan(posterior)) |>
  summarise(marginal_posterior = sum(posterior),
            marginal_prior = sum(prior),
            .groups = "drop") |>
  ggplot() +
    geom_line(aes(x = K, y = marginal_posterior)) +
    geom_line(aes(x = K, y = marginal_prior), col = "blue") +
    labs(y = "Probability", x = "K") +
    theme_bw()

p2 <- grid |>
  group_by(r) |>
  filter(!is.nan(posterior)) |>
  summarise(marginal_posterior = sum(posterior),
            marginal_prior = sum(prior),
            .groups = "drop") |>
  ggplot() +
  geom_line(aes(x = r, y = marginal_posterior)) +
  geom_line(aes(x = r, y = marginal_prior), col = "blue") +
  labs(y = "Probability", x = "r") +
  theme_bw()

p3 <- grid |>
  group_by(theta) |>
  filter(!is.nan(posterior)) |>
  summarise(marginal_posterior = sum(posterior),
            marginal_prior = sum(prior),
            .groups = "drop") |>
  ggplot() +
  geom_line(aes(x = theta, y = marginal_posterior)) +
  geom_line(aes(x = theta, y = marginal_prior), col = "blue") +
  labs(y = "Probability", x = "theta") +
  theme_bw()

p4 <- grid |>
  group_by(sigma) |>
  filter(!is.nan(posterior)) |>
  summarise(marginal_posterior = sum(posterior),
            marginal_prior = sum(prior),
            .groups = "drop") |>
  ggplot() +
  geom_line(aes(x = sigma, y = marginal_posterior)) +
  geom_line(aes(x = sigma, y = marginal_prior), col = "blue") +
  labs(y = "Probability", x = "sigma") +
  theme_bw()

plot_grid(p1, p2, p3, p4, nrow = 1)

rm(p1, p2, p3, p4)
```

Evidence of bimodality (theta logistic not flexible enough to fit the density dependence we see in our experiment), which is also obvious in the plots below.

We can also randomly draw parameter combinations from the posterior distribution (weighted by their posterior probabilities) and then plot the resulting estimated curve for each of them:

```{r, fig.width=4, fig.height=4}
p <- ggplot() +
  geom_line() +
  labs(y = "Nhat", x = "g") +
  theme_bw()

is <- sample(1:nrow(grid), 100, replace = TRUE, prob = grid$posterior)
for (i in is) {
  smpl <- tibble(g_true = seq(0, 1.1, by = 0.001)) |>
    rowwise() |>
    mutate(Neq_smpl = Neq(grid$K[i], g_true, grid$r[i], grid$theta[i]))
  p <- p + geom_line(aes(g_true, Neq_smpl), data = smpl, col = "grey", alpha = 0.1)
}

p <- p +
  geom_point(aes(g, Neq_dat), data = exp_data)
  
print(p)

rm(p, smpl, is, i)
```

Median prediction (based on median parameter values) and 95% credible intervals (based on predictions from posterior predictive distribution). Option to use HDI or quantile based credible intervals, very similar results but more jagged errors around the prediction.  

```{r, fig.width=4, fig.height=4}
# Step 1: Set up g values and sample from posteriors (store indices of samples)
g_seq <- seq(0, 1.1, length = 200)
n_samples <- 10000
sample_indices <- sample(1:nrow(grid), n_samples, replace = TRUE, prob = grid$posterior)


#### as an aside get the predictions based on median/mean parameter values ..
samples <- grid[sample_indices, ]
median_pred <- tibble(g = g_seq) |>
  rowwise() |>
  mutate(Neq = Neq(median(samples$K), g, median(samples$r), median(samples$theta)))
mean_pred <- tibble(g = g_seq) |>
  rowwise() |>
  mutate(Neq = Neq(mean(samples$K), g, mean(samples$r), mean(samples$theta)))

# ... or draw that has the highest posterior probability
max_prob_draw <- grid[which.max(grid$posterior), ]
max_prob_pred <- tibble(g = g_seq) |>
  rowwise() |>
  mutate(Neq = Neq(max_prob_draw$K, g, max_prob_draw$r, max_prob_draw$theta))


# Step 2: Compute Neq for each sample across all g
# This will generate a long-format data frame with each g and each sample's predicted Neq
all_samples_df <- map_dfr(sample_indices, function(i) {
  Neq_vals <- map_dbl(g_seq, function(g_val) {
    Neq(grid$K[i], g_val, grid$r[i], grid$theta[i])
  })
  
  tibble(
    g = g_seq,
    Neq = Neq_vals,
    sample_id = i
  )
})

# Step 3: Summarise posterior predictions at each g
## quantile credible intervals
credible_df <- all_samples_df |>
  group_by(g) |>
  summarise(
    Neq_lower = quantile(Neq, 0.025, na.rm = TRUE),
    Neq_median = median(Neq, na.rm = TRUE),
    .groups = "drop",
    Neq_upper = quantile(Neq, 0.975, na.rm = TRUE),
  ) 

## HDI credible intervals
credible_df2 <- all_samples_df |>
  group_by(g) |>
  summarise(
    hdi_vals = list(hdi(Neq, credMass = 0.95)),
    Neq_median = median(Neq, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    Neq_lower = map_dbl(hdi_vals, ~ .x[1]),
    Neq_upper = map_dbl(hdi_vals, ~ .x[2])
  ) |>
  select(-hdi_vals)



# Step 4: Plot the credible intervals
ggplot() +
  geom_ribbon(data = credible_df, aes(x = g, ymin = Neq_lower, ymax = Neq_upper), 
              fill = "grey80", alpha = 0.5) +
  geom_line(data = credible_df, 
            aes(x = g, y = Neq_median), color = "black") +
  geom_point(data = exp_data, aes(x = g, y = Neq_dat), size = 1.5) + 
  geom_line(data = max_prob_pred, 
            aes(x = g, y = Neq), col = "red", linetype = "dashed") +
  geom_line(data = median_pred, 
            aes(x = g, y = Neq), col = "blue", linetype = "dashed") +
  geom_line(data = mean_pred, 
            aes(x = g, y = Neq), col = "orange", linetype = "dashed") +  
  labs(x = "g", y = "N") +
  theme_bw()

rm(all_samples_df, g_seq, n_samples, sample_indices, mean_pred, 
   max_prob_draw, max_prob_pred, samples, Neq)

```

Median from posterior predictive distribution and the prediction based on median parameters almost perfectly overlap, so plot the inverse theta logistic with median parameter values for simplicity.


Estimated theta 

```{r}
# Sample indices from grid weighted by posterior
samples <- sample(1:nrow(grid), 10000, replace = TRUE, prob = grid$posterior)

# Extract theta values for those samples
theta_samples <- grid$theta[samples]

# Calculate median and 95% credible intervals from the samples
median_theta <- median(theta_samples)
lower_theta <- quantile(theta_samples, 0.025)
upper_theta <- quantile(theta_samples, 0.975)

hdi_theta <- hdi(theta_samples, credMass = 0.95)

cat(sprintf("Predicted theta was %.3f (95%% HDI: %.3f - %.3f)\n",
            median_theta, hdi_theta[1], hdi_theta[2]))

cat(sprintf("Predicted theta was %.3f (95%% credible interval: %.3f - %.3f)\n",
            median_theta, lower_theta, upper_theta))

rm(samples, theta_samples, median_theta, lower_theta, upper_theta, hdi_theta)
```

Estimated r and K 

```{r}
# Sample indices from grid weighted by posterior
samples <- sample(1:nrow(grid), 10000, replace = TRUE, prob = grid$posterior)

# Extract r and K values for those samples
r_samples <- grid$r[samples]
K_samples <- grid$K[samples]

## r ## 
median_r <- median(r_samples)
lower_r <- quantile(r_samples, 0.025)
upper_r <- quantile(r_samples, 0.975)
hdi_r <- hdi(r_samples, credMass = 0.95)
cat(sprintf("Predicted r was %.3f (95%% HDI: %.3f - %.3f)\n",
            median_r, hdi_r[1], hdi_r[2]))
cat(sprintf("Predicted r was %.3f (95%% credible interval: %.3f - %.3f)\n",
            median_r, lower_r, upper_r))

## K ## 
median_K <- median(K_samples)
lower_K <- quantile(K_samples, 0.025)
upper_K <- quantile(K_samples, 0.975)
hdi_K <- hdi(K_samples, credMass = 0.95)
cat(sprintf("Predicted K was %.3f (95%% HDI: %.3f - %.3f)\n",
            median_K, hdi_K[1], hdi_K[2]))
cat(sprintf("Predicted K was %.3f (95%% credible interval: %.3f - %.3f)\n",
            median_K, lower_K, upper_K))

rm(samples, r_samples, K_samples, median_r, lower_r, upper_r, hdi_r,
   median_K, lower_K, upper_K, hdi_K)
```




Figure 1B

```{r, fig.height=8, fig.width=8}
# extract median parameter estimates from inverse theta model
samples <- grid[sample(1:nrow(grid), 10000, replace = TRUE, prob = grid$posterior), ]
median_theta <- median(samples$theta)
median_K <- median(samples$K)
median_r <- median(samples$r)

# inverse theta logistic with median parameter estimates
g_seq <- seq(0, median_r, length = 500)
N_curve <- median_K * (1 - (g_seq / median_r)) ^ (1 / median_theta)
curve_data <- data.frame(g = g_seq, N = N_curve)

# plot the experimental data with inverse theta and also model predictions
ggplot(data = exp_data, aes(y = Neq_dat, x = g)) +
  labs(
    y = "Optical Density", x = "Dilution rate (ml/hour/ml)") +

  geom_ribbon(data = credible_df, aes(x = g, ymin = Neq_lower, ymax = Neq_upper), 
              inherit.aes = FALSE, fill = j_green, alpha = 0.4, color = NA) +

  geom_line(data = curve_data, 
            aes(x = g, y = N), 
            col = j_green, size = 2, linetype = "dashed") +
  
  geom_line(
    data = prediction_posteriors,
    aes(y = N_star, x = dilution_rate, group = draw),
    color = j_green, size = 0.25, linetype = "solid", alpha = 0.1) +
  
  geom_line(
    data = prediction,
    aes(y = N_star, x = dilution_rate),
    color = j_green, size = 1, linetype = "solid", alpha = 1) +
  
  geom_point(colour = j_green, size = 5, alpha = 0.8) +
  
  theme_minimal(base_size = 20) +
  
  theme(panel.grid = element_blank(),
            panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5)) +
  
  xlim(0, 1)
  

```


```{r, fig.height=4, fig.width=4}

# plot the theta logistic with median parameter estimates (for density dependence perspective)
od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- median_r * (1 - (od_seq / median_K)^ median_theta)
curve_data <- data.frame(od_blanked = od_seq, dilution_rate = dilution_curve) %>%
  filter(dilution_rate > 0)

# plot
ggplot(exp_data, aes(x = od_blanked, y = dilution_rate)) +
  labs(
    x = "Density",
    y = "Growth"
  ) +
  geom_line(
    data = curve_data,
    aes(x = od_blanked, y = dilution_rate),
    color = j_blue, size = 1.5, linetype = "dashed") +
  scale_x_continuous(limits = c(0, 0.2), breaks = scales::pretty_breaks(n = 3)) +
  geom_point(colour = j_blue, size = 5, alpha = 0.3) + 
  theme_light(base_size = 20)  +
  theme(panel.grid = element_blank(),
            panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5))

# clean up environment 
rm(median_theta, median_K, median_r, dilution_curve, od_seq, exp_data, 
   curve_data, grid, median_pred, samples, g_seq, N_curve, credible_df, credible_df2)

```





## Predictions across resource dynamics

### Chemostat resources

Same model as before, except now m is not equal to d. 

$$
\frac{dN}{dt}  = N( \frac{\mu_{max}R}{k + R} -m) \\
\frac{dR}{dt} = d(S - R) - \frac{\mu_{max}R}{k + R}QN
$$

From above, we know that the analytical expression perfectly predicts the simulations so we can just use that. 

$$N^* = \frac{d}{\mu Q} \left [\frac{\mu S}{m}  - \frac{mk}{\mu - m} - k \right ]$$

```{r, fig.width=4, fig.height=4}
# parameters 
u <- mu_mean
k <- k_mean
q <- q_mean
S <- 0.05

# generate predictions for different m and d
# m from near 0 to maximum growth rate 
# three values of d across the range used in our system
m_vals <- seq(0.01, u, length = 1000) 
dil_vals <- c(0.1, 0.5, 1)
df_chemo <- expand.grid(m = m_vals, dil = dil_vals) %>%
  mutate(N = N_star_chemo(dil, m, u, S, k, q)) %>%
  filter(N > 0)

# generate uncertainty 
# new values of m to account for varying mu_max across draws
m_vals <- seq(0.01, max(posterior_subset$mu_max), length = 1000) 
df_chemo_posts <- posterior_subset %>%
  select(draw, mu_max, Ks, q) %>%
  crossing(m = m_vals, dil = dil_vals) %>%
  mutate(N = N_star_chemo(dil, m, mu_max, S, Ks, q))

# remove observations where m goes above mu_max in each draw
df_chemo_posts <- df_chemo_posts %>%
  filter(m < mu_max) 

# density dependence plots
ggplot() +
  geom_line(data = df_chemo, aes(y = m, x = N, group = dil, colour = dil)) +
  geom_line(data = df_chemo_posts, 
            aes(y = m, x = N, group = interaction(dil, draw), colour = dil),
            alpha = 0.1, size = 0.5) + 
  labs(y = "Growth rate", x = "N*", color = "") +
  xlim(0, 0.5) +
  ylim(0, 0.7) + 
  geom_hline(yintercept = c(0.1, 0.3, 0.5), linetype = "dashed", size = 0.5) +
  theme_minimal()

# clean up environment
rm(dil_vals, k, m_vals, q, S, u)
```

The uncertainty around the Monod parameters only influences the density dependence predictions at low densities - after the inflection point, the chemostat dynamics dominate. 

The shape of density dependence will really depend on the underlying mortality of the system (separate form the harvest rate that is being manipulated in the Abram's approach). If there is no mortality, then per capita growth is just the Monod function. Adding in baseline mortality will change the window of density dependence that is being viewed. The parts of the density dependence curves above the dashed lines show you density dependence under three different baseline mortalities (for the three dilutions).

Plot these 9 scenarios separately to avoid any visual biases. 

```{r, fig.width=5, fig.height=5}
# Function to generate the plot
plot_subset <- function(dilution, mort) {
  
  # prediction with mean parameters
  mean <- df_chemo %>% 
    filter(dil == dilution) %>%
    filter(N > 0) %>%
    filter(m > mort)
  
  # predictions for each posterior draw
  posteriors <- df_chemo_posts %>%
    filter(dil == dilution) %>%
    filter(N > 0) %>%
    filter(m > mort)
  
  # plot together
  p <- ggplot() +
    geom_line(data = mean, aes(y = m, x = N), size = 1) +
    geom_line(data = posteriors, 
              aes(y = m, x = N, group = interaction(dil, draw)), 
              alpha = 0.1, size = 0.5) +
    labs(y = "", x = "", title = paste("D =", dil, " M =", mort)) +
    theme_minimal() +
    theme(plot.title = element_text(size = 10))
  
  return(p)  # Return the plot
}

# List to collect all plots
plot_list <- list()

# Loop over three dilutions and three baseline mortalities
for (dil in c(0.1, 0.5, 1)) {
  for (mort in c(0.1, 0.3, 0.5)) {
    p <- plot_subset(dil, mort)  # Generate the plot
    plot_list[[paste0("dil_", dil, "_mort_", mort)]] <- p  # Store the plot in the list
  }
}

plot_grid(plotlist = plot_list, ncol = 3, nrow = 3)
  
rm(df_chemo, df_chemo_posts, p, plot_list, dil, mort, plot_subset)
  
```

Dilution rate only changes the x axis range so we can just plot one dilution rate - choose the one that aligns closest to our experiment in terms of the ODs that can be achieved in our reactors. 


### Logistic resources

Now we have the consumer-resource model: 

$$
\frac{dN}{dt}  = N( \frac{\mu_{max}R}{k + R} -m) \\
\frac{dR}{dt} = r R (1 - \frac{R}{K}) - \frac{\mu_{max}R}{k + R}QN
$$
Instead of $S$ and $d$, we have $r$ (intrinsic growth rate of resource) and $K$ (carrying capacity of resource). 

Again, we can use an analytical solution to predict the equilibrium density of the consumer for this particular model. 

$$
N^* = \frac{k r \left( -((k + K) m) + K \mu \right)}{(K q) (m - \mu)^2}
$$


```{r}
N_star_log <- function(r, K, m, u, k, q) {
  return(((k*r)*(-((k + K)*m) + K*u))/((K*q)*(m - u)^2))
}
```


```{r, fig.width=4, fig.height=4}
# parameters 
u <- mu_mean
k <- k_mean
q <- q_mean

# generate data
m_vals <- seq(0, u, length.out = 1000) 
r <- 100
K_vals <- c(0.0004, 0.0003, 0.0002, 0.0001)
df_log <- expand.grid(m = m_vals, K = K_vals) %>%
  mutate(N = N_star_log(r, K, m, u, k, q))

# generate uncertainty 
m_vals <- seq(0, max(posterior_subset$mu_max), length = 1000) 
df_log_posts <- posterior_subset %>%
  select(draw, mu_max, Ks, q) %>%
  crossing(m = m_vals, K = K_vals) %>%
  mutate(N = N_star_log(r, K, m, mu_max, Ks, q))


ggplot() +
  geom_point(data = df_log_posts, 
             aes(y = m, x = N, group = interaction(K, draw), colour = K),
             alpha = 0.02, size = 0.25) +
  geom_line(data = df_log, aes(y = m, x = N, group = K, color = K)) +
  geom_line(size = 1.5) +
  labs(y = "Growth rate", x = "N*") +
  xlim(0, NA) +
  ylim(0, 0.4) +
  theme_minimal()

rm(df_log, df_log_posts, k, K_vals, m_vals, q, r, u)
```
It is very hard to find the right parameter values for the logistic growth that lead to fixed point equilibria (i.e., no extinctions and no cycles) over a wide range of mortality values. 

The quota and r determine the equilibrium abundance but don't change the shape of density dependence. Keeping the Monod function fixed, it is the carrying capacity of the logistic model that controls the shape of density dependence. This just controls which part of the functional response is being observed really (i.e., which part of the Monod curve lies above 0). Pick one of these and vary mortality to show the same effect. Increasing K too much leads to cycles at low moralities. Decreasing K too much leads to extinction at high moralities. Very fine range of parameters. 

```{r, fig.width=3, fig.height=3, echo = FALSE, include = FALSE}
###################################
#### Simulate for one dilution ####
###################################

### set initial conditions and paramaters
t <- seq(0, 100, 1)
initial_conditions <- c(N = 0.01, R = 0.01)

params <- list(
  mu_max = mu_mean,  
  k = k_mean,          
  Q = q_mean,        
  m = 0,
  r = 100,
  K = 0.0004
  )

# Run model 
output <- ode(
    y = initial_conditions,
    times = t,
    func = cr_tII_log,
    parms = params
  )

# Convert output to a data frame
output_df <- as.data.frame(output)

# Plot dynamics
ggplot(output_df, aes(x = time)) +
  geom_line(aes(y = N, color = "N")) +
  geom_line(aes(y = R, color = "R")) +
  labs(color = "") +
  xlim(0, 100) +
  theme_minimal() 


rm(output, output_df, params, initial_conditions, t)

```



### Pulsed resources

The Abrams method (Growth-Density inversion) doesn't work so well here as the system doesn't ever reach a fixed point equilibrium (pulsed resources lead to cycling dynamics for the consumer). Instead we can get per capita growth rates from the model directly and plot these against consumer density. 

Growth under a single pulse of resources should lead to superlinear density dependence for type II consumers. Add resources at the start, and watch the consumers grow and the resources go down. 


```{r}
# Set initial conditions and parameters
t <- seq(0, 5, by = 0.001)  
initial_conditions <- c(N = 0.01, R = 0.05)

# Fixed parameters 
params <- list(
  mu_max = mu_mean,     
  k = k_mean,           
  Q = q_mean,        
  m = 0 
)

# Prediction with mean estimates
# (cr_tII_pulse has no resource supply, initial resources only)
  output <- ode(
    y = initial_conditions,
    times = t,
    func = cr_tII_pulse,
    parms = params
    )
  
# Calculate per-capita growth rate at each timepoint 
# from consumer-resource model: growth rate = Monod - mortality
output_df <- as.data.frame(output)
output_df <- output_df %>%
  mutate(pcgr = (params$mu_max * R / (params$k + R)) - params$m)

# Incorporate uncertainty by looping over posteriors 
sim_results <- posterior_subset %>%
  select(draw, mu_max, Ks, q) %>%
  mutate(m = 0) %>%
  
  # for each row in posterior_subset (100 draws)
  # simulate the cr_t_II_pulse model 
  # and calculate per-capita growth rate
  # then add all these results to a big dataframe
  # which will contain the draw number 
  # and the pcgr for each draw at each timepoint
  pmap_dfr(function(draw, mu_max, Ks, q, m) {
    params <- list(mu_max = mu_max, k = Ks, Q = q, m = m)
    ode_out <- ode(
      y = initial_conditions,
      times = t,
      func = cr_tII_pulse,  
      parms = params
    )
    as.data.frame(ode_out) %>%
      mutate(
        draw = draw,
        pcgr = (mu_max * R / (Ks + R)) - m  
      )
  })


# Plot dynamics
p1 <- ggplot(output_df, aes(x = time)) +
  geom_line(aes(y = N, color = "N")) +
  geom_line(aes(y = R, color = "R")) +
  labs(color = "") +
  theme_minimal() 

# Plot per-capita growth rate (group = draw is important)
p2 <- ggplot() +
  geom_line(data = sim_results, aes(x = N, y = pcgr, group = draw),
            alpha = 0.05, size = 0.5) +
  geom_line(data = output_df, aes(x = N, y = pcgr)) +
  geom_line() +
  theme_minimal()

plot_grid(p1, p2)

rm(output, output_df, p1, p2, params, sim_results, initial_conditions, t)
```
If we use multiple pulses the same results hold.

The interval between pulses doesn't matter. Even short intervals where the resources aren't depleted fully between pulses doesn't turn the system into a chemostat style resources. There is no resource loss here other than through the consumers. Therefore there is no density dependence of resources for the consumers to "inherit". 


```{r, echo = FALSE, include = FALSE}

# hyperparameters 
total_time <- 100
pulse_intervals <- 10
resolution <- 0.001
pulse_size <- 0.05
m <- 0.1

# Initial conditions
initial_conditions <- c(N = 0.01, R = 0.05)

# Time frame
t <- seq(0, total_time, by = resolution)

# Define pulse times 
pulse_times <- seq(0, total_time, by = pulse_intervals)

# Set parameters
params <- list(
  mu_max = mu_mean,
  k = k_mean,
  Q = q_mean,
  pulse_size = pulse_size,
  m = m  
)

# Run the model with events
output <- ode(
  y = initial_conditions,
  times = t,
  func = cr_tII_pulse,
  parms = params,
  events = list(func = pulse_fun, times = pulse_times),
  method = "ode45"
)

# Convert to data frame
output_df <- as.data.frame(output)

# Calculate per-capita growth rate at each timestep
output_df <- output_df %>%
  mutate(pcgr = (params$mu_max * R / (params$k + R)) - params$m)

# Plot dynamics
p1 <- ggplot(output_df, aes(x = time)) +
  geom_line(aes(y = N, color = "N")) +
  geom_line(aes(y = R, color = "R")) +
  labs(color = "") +
  theme_minimal() 

# Plot per capita growth rate once system is stable
output_df_stable <- output_df %>%
  filter(time > 50)

p2 <- ggplot(output_df_stable, aes(x = N, y = pcgr)) +
  geom_point() +
  theme_minimal()

plot_grid(p1, p2)

rm(output, output_df, p1, p2, params, initial_conditions, t,
   output_df_stable, m, pulse_intervals, pulse_size, pulse_times,
   resolution, total_time)
```



### Figure 1C

The goal here is to create a 3 x 3 panel plot where the three resource dynamics (chemostat, logistic, pulsed) are crossed with three levels of mortality. 

As the three resources dynamics lead to three different observed maximum growth rates, we can't give all three the same exact mortality values to represent high, medium, and low mortalities. A straightforward solution is to express mortality as a percentage of the observed maximum growth rate (from the Monod). Choosing something like 25%, 50%, and 75%  mortality (relative to max growth) is the best way to show the full range of impacts that baseline mortality has on the observed density dependence. 

The choices of dilution rate for the chemostat model and carrying capacity of resource for the logistic model are somewhat arbitrary. Here we will select the "middle" dilution rate that we tested in our experiment, which produces OD equilibrium densities for different harvest rates that are feasible in our system. For the logistic model, we'll select the highest carrying capacity that is possible before the system goes into cycles so that the densities of the consumers (OD) are closest to those seen in our experiments. 

Collect the data first (chemostat and logistic have analytical solutions, pulsed needs to be simulated).

```{r}
# parameters 
u <- mu_mean
k <- k_mean
q <- q_mean

#### data for chemostat resources ####
m_vals <- seq(0, u, length.out = 10000) 
dil_vals <- 0.5      # fixed dilution rate 
S <- 0.05
df_chemo <- expand.grid(m = m_vals, dil = dil_vals) %>%
  mutate(N = N_star_chemo(dil, m, u, S, k, q)) %>%
  mutate(pcgr = m) %>%
  filter(N > 0) 
# generate uncertainty 
m_vals <- seq(0.01, max(posterior_subset$mu_max), length = 1000) 
df_chemo_posts <- posterior_subset %>%
  select(draw, mu_max, Ks, q) %>%
  crossing(m = m_vals, dil = dil_vals) %>%
  mutate(N = N_star_chemo(dil, m, mu_max, S, Ks, q))
df_chemo_posts <- df_chemo_posts %>%
  filter(m < mu_max) %>%
  filter(N > 0) %>%
  mutate(pcgr = m)


#### data for logistic resources  ####
m_vals <- seq(0, u, length.out = 10000) 
r <- 100            # just controls x axis range
K_vals <- 0.0004    # fixed carrying capacity of resource (any higher and we get cycles)
df_log <- expand.grid(m = m_vals, K = K_vals) %>%
  mutate(N = N_star_log(r, K, m, u, k, q)) %>%
  mutate(pcgr = m) %>%
  filter(N > 0)
# generate uncertainty 
m_vals <- seq(0, max(posterior_subset$mu_max), length = 1000) 
df_log_posts <- posterior_subset %>%
  select(draw, mu_max, Ks, q) %>%
  crossing(m = m_vals, K = K_vals) %>%
  mutate(N = N_star_log(r, K, m, mu_max, Ks, q)) %>%
  filter(N > 0) %>%
  mutate(pcgr = m)


#### data for pulsed resource ####
t <- seq(0, 5, length.out = 10000)  
initial_conditions <- c(N = 0.01, R = 0.05)
params <- list(
  mu_max = mu_mean,     
  k = k_mean,           
  Q = q_mean,        
  m = 0
)
output <- ode(
  y = initial_conditions,
  times = t,
  func = cr_tII_pulse,
  parms = params
    )
output_df <- as.data.frame(output)
df_pulse <- output_df %>%
  mutate(pcgr = (params$mu_max * R / (params$k + R)) - params$m) %>%
  filter(N > 0)
# generate uncertainty 
df_pulse_posts <- posterior_subset %>%
  select(draw, mu_max, Ks, q) %>%
  mutate(m = 0) %>%
  pmap_dfr(function(draw, mu_max, Ks, q, m) {
    params <- list(mu_max = mu_max, k = Ks, Q = q, m = m)
    ode_out <- ode(
      y = initial_conditions,
      times = t,
      func = cr_tII_pulse,  # your ODE function
      parms = params
    )
    as.data.frame(ode_out) %>%
      mutate(
        draw = draw,
        pcgr = (mu_max * R / (Ks + R)) - m  # per-capita growth
      )
  })

# clean up environment
rm(dil_vals, initial_conditions, k, K_vals, m_vals, q, r, S, t, 
   u, output, output_df, params)
```


Create the nine plots 

```{r, fig.width=6.3, fig.height=6}
# Function to generate the plot
plot_subset <- function(data, data2, mort_value) {
  
  mean <- data %>% 
    filter(pcgr > mort_value) %>%
    # Monod growth rate minus mortality = realised growth rate 
    mutate(pcgr_realised = pcgr - mort_value)
  
  posteriors <- data2 %>%
    filter(pcgr > mort_value) %>%
    mutate(pcgr_realised = pcgr - mort_value)


  p <- ggplot() +
    geom_line(data = posteriors, aes(y = pcgr_realised, x = N, group = draw), 
              alpha = 0.05, size = 0.5, colour = j_blue) +
    geom_line(data = mean, aes(y = pcgr_realised, x = N), colour = j_blue) +
    labs(y = "", x = "") +
    theme_minimal() +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 4)) +
    scale_y_continuous(limits = c(0, NA), breaks = scales::pretty_breaks(n = 4)) +
    theme(panel.grid = element_blank(),
            panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.5))
  return(p) 
}

# List to collect all plots
plot_list <- list()
n <- 1  # only initialize once

# list of datasets to use
datasets <- list(df_chemo, df_pulse, df_log)
datasets2 <- list(df_chemo_posts, df_pulse_posts, df_log_posts)

# Mortality fractions
mort_fracs <- c(0.25, 0.50, 0.75)

# Loop over datasets and mortality levels
for (i in seq_along(datasets)) {
  data <- datasets[[i]]
  data2 <- datasets2[[i]]
  max_growth <- max(data$pcgr, na.rm = TRUE)

  for (mort in mort_fracs) {
    mort_value <- mort * max_growth
    p <- plot_subset(data, data2, mort_value)
    plot_list[[n]] <- p
    n <- n + 1
  }
}

# Display all plots
custom_order <- c(3, 6, 9, 2, 5, 8, 1, 4, 7)
plot_list_ordered <- plot_list[custom_order]
plot_grid(plotlist = plot_list_ordered, ncol = 3, nrow = 3)

# clean up environment 
rm(data, data2, datasets, datasets2, p, plot_list, plot_list_ordered,
   i, custom_order, mort, mort_fracs, mort_value, n, max_growth, plot_subset)

```


It is also possible to have just three plots (one for each mortality, or one for each resource dynamic) that each have three lines (one for each resource dynamic, or one for each mortality). To do this, we'll need to rescale the axes. Y axis could be scaled between maximum observed per capita growth and mortality. X axis could be scaled between 0 and carrying capacity. The problem with this approach is that it collapses all of the posteriors on top of each other so the uncertainty is hidden. 


```{r, fig.height=3, fig.width=9, echo = FALSE, include = FALSE}
mort_plot <- function(mort) {

  chemo_max <- max(df_chemo$pcgr, na.rm = TRUE)
  chemo_mort_value <- mort * chemo_max
  chemo_plot <- df_chemo %>%
    filter(pcgr > chemo_mort_value) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE))
  
  log_max <- max(df_log$pcgr, na.rm = TRUE)
  log_mort_value <- mort * log_max
  log_plot <- df_log %>%
    filter(pcgr > log_mort_value) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE))
  
  pulse_max <- max(df_pulse$pcgr, na.rm = TRUE)
  pulse_mort_value <- mort * pulse_max
  pulse_plot <- df_pulse %>%
    filter(pcgr > pulse_mort_value) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE))
  
  
  
  ## posteriors 
  chemo_p_plot <- df_chemo_posts %>%
    group_by(draw) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    filter(pcgr_rs > mort) %>%
    mutate(pcgr_rs_2 = (pcgr_rs - min(pcgr_rs, na.rm = TRUE)) / (max(pcgr_rs, na.rm = TRUE) - min(pcgr_rs, na.rm = TRUE))) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE)) %>%
    ungroup() 
  
  log_p_plot <- df_log_posts %>%
    group_by(draw) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    filter(pcgr_rs > mort) %>%
    mutate(pcgr_rs_2 = (pcgr_rs - min(pcgr_rs, na.rm = TRUE)) / (max(pcgr_rs, na.rm = TRUE) - min(pcgr_rs, na.rm = TRUE))) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE)) %>%
    ungroup() 
    
  pulse_p_plot <- df_pulse_posts %>%
    group_by(draw) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    filter(pcgr_rs > mort) %>%
    mutate(pcgr_rs_2 = (pcgr_rs - min(pcgr_rs, na.rm = TRUE)) / (max(pcgr_rs, na.rm = TRUE) - min(pcgr_rs, na.rm = TRUE))) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE)) %>%
    ungroup() 
  
  
  
  
  ggplot() +
    
    geom_line(data = chemo_p_plot, aes(x = avg_od_rs, y = pcgr_rs_2, group = draw), 
              size = 0.5, color = "cyan3", alpha = 0.1) +
    geom_line(data = log_p_plot, aes(x = avg_od_rs, y = pcgr_rs_2, group = draw), 
              size = 0.5, color = "darkorange" , alpha = 0.1) +
    geom_line(data = pulse_p_plot, aes(x = avg_od_rs, y = pcgr_rs_2, group = draw), 
              size = 0.5, color = "maroon", alpha = 0.1) +
    
    #geom_line(data = chemo_plot, aes(x = avg_od_rs, y = pcgr_rs), 
    #          size = 1.5, color = "cyan3") +
    #geom_line(data = log_plot, aes(x = avg_od_rs, y = pcgr_rs), 
    #          size = 1.5, color = "darkorange") +
    #geom_line(data = pulse_plot, aes(x = avg_od_rs, y = pcgr_rs), 
    #          size = 1.5, color = "maroon") +
    labs(y = "", x = "") +
    theme_minimal() +
    theme(legend.position = "none")
}


  
p1 <- mort_plot(0.25) 
p2 <- mort_plot(0.5) 
p3 <- mort_plot(0.75) 


  
# Display all plots
plot_grid(p1, p2, p3, ncol = 3, nrow = 1)
  
```


```{r, fig.height=3, fig.width=9, include = FALSE, echo = FALSE}

rescale_plot <- function(mean_data, posts_data) {
  
  mort <- c(0.25, 0.5, 0.75)
  
  ## mean 

  
  ## posteriors 
  p_plot_1 <- posts_data %>%
    group_by(draw) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    filter(pcgr_rs > mort[1]) %>%
    mutate(pcgr_rs_2 = (pcgr_rs - min(pcgr_rs, na.rm = TRUE)) / (max(pcgr_rs, na.rm = TRUE) - min(pcgr_rs, na.rm = TRUE))) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE)) %>%
    ungroup() 
  
  p_plot_2 <- posts_data %>%
    group_by(draw) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    filter(pcgr_rs > mort[2]) %>%
    mutate(pcgr_rs_2 = (pcgr_rs - min(pcgr_rs, na.rm = TRUE)) / (max(pcgr_rs, na.rm = TRUE) - min(pcgr_rs, na.rm = TRUE))) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE)) %>%
    ungroup() 
  
  p_plot_3 <- posts_data %>%
    group_by(draw) %>%
    mutate(pcgr_rs = pcgr / max(pcgr, na.rm = TRUE)) %>%
    filter(pcgr_rs > mort[3]) %>%
    mutate(pcgr_rs_2 = (pcgr_rs - min(pcgr_rs, na.rm = TRUE)) / (max(pcgr_rs, na.rm = TRUE) - min(pcgr_rs, na.rm = TRUE))) %>%
    mutate(avg_od_rs = N / max(N, na.rm = TRUE)) %>%
    ungroup() 
  
  
  
  ggplot() +
    geom_line(data = p_plot_1, aes(x = avg_od_rs, y = pcgr_rs_2, group = draw), 
              size = 0.5, color = "gray10", alpha = 0.1) +
    geom_line(data = p_plot_2, aes(x = avg_od_rs, y = pcgr_rs_2, group = draw), 
              size = 0.5, color = "gray40", alpha = 0.1) +
    geom_line(data = p_plot_3, aes(x = avg_od_rs, y = pcgr_rs_2, group = draw), 
              size = 0.5, color = "gray70", alpha = 0.1) +
    labs(y = "", x = "") +
    theme_minimal() +
    theme(legend.position = "none")
}
  

p1 <- rescale_plot(df_chemo, df_chemo_posts)
p2 <- rescale_plot(df_log, df_log_posts)
p3 <- rescale_plot(df_pulse, df_pulse_posts)



  
# Display all plots
plot_grid(p1, p2, p3, ncol = 3, nrow = 1)
```



