---
title: "chemo-dd"
subtitle: "Supplementary models"
output: 
  html_notebook:
    toc: true
    toc_depth: 4
author: "James Orr"
---

This notebook uses the processed data from `1-data-prep.Rmd` for supplementary models. We explore various approaches to quantify the curvature of the density-dependence.

### Set up environment 

Load packages and clear environment 

```{r}
#### Required packages
library(tidyverse)  
library(brms)
library(cowplot)

#### Clear  environment 
rm(list = ls())   
```

```{r}
all_samples <- read.csv("data/processed/all_samples.csv")
equilibrium_samples <- read.csv("data/processed/equilibrium_samples.csv")
chibio <- read.csv("data/processed/chibio.csv")

# "final" datasets
chemo_dd_eqs_av <- read.csv("data/processed/equilibrium_samples_av.csv")
chemo_dd_eqs_last <- read.csv("data/processed/equilibrium_samples_last.csv")
chemo_dd_eqs_first <- read.csv("data/processed/equilibrium_samples_first.csv")
chemo_dd_eqs_day1 <- read.csv("data/processed/equilibrium_samples_day1.csv")
chemo_dd_eqs_day2 <- read.csv("data/processed/equilibrium_samples_day2.csv")
chemo_dd_eqs_day3 <- read.csv("data/processed/equilibrium_samples_day3.csv")
chemo_dd_eqs_day4 <- read.csv("data/processed/equilibrium_samples_day4.csv")

theory_predictions <- read.csv("data/theory/prediction.csv")
theory_predictions2 <- read.csv("data/theory/prediction2.csv")

```

For the exploratory analyses here I'll use `chemo_dd_eqs_av`, which we believe is the most sensible way of selecting equilibrium points. Once our modelling framework is decided, we'll run the analyses with all the different "final" datasets. 


### Flipped theta-logistic model 

For this first attempt I'll use OD and dilution rate from `equilibrium_samples` - 30 observations, no pseudoreplication. `experiment_replicate` may be a grouping factor that I can use for multi-level models. We manipulated dilution rate (we can be confident that there is no/low error around these values) and OD is our response variable. This means we are flipping the theta-logistic model on its head. 

The standard theta-logistic model is: 

$$
\frac{1}{N} \frac{d N}{d t}=r\left(1-\left(\frac{N}{K}\right)^\theta\right)
$$
Where $\frac{1}{N} \frac{d N}{d t} = g$ is the per capita growth (i.e. `dilution_rate`), $N$ is the abundance/density (i.e., `od_blanked`), $r$ is the intrinsic growth rate, $K$ is the carrying capacity, and $\theta$ is the parameter that controls the shape of density dependence. 
When $\theta$ is 1, it is linear (i.e., logistic growth model), when it is >1 the density dependence is concave (what we expect based on consumer-resource theory), when it is <1 the density dependence is sublinear (a hot topic at the moment). 

We will be using `brms` to estimate the values of $r$, $K$, and most importantly $\theta$, based on the relationship between $g$ and $N$. However, as we manipulated per capita growth, we need to express the theta-logistic model in terms of $N$ (our response variable given our experimental design). 

Starting from

$$
g=r\left(1-\left(\frac{N}{K}\right)^\theta\right)
$$
we can rearrange to

$$
N=K\left(1-\frac{g}{r}\right)^{\frac{1}{\theta}}
$$


#### First attempt with brms

Will use Gamma error distribution with the identity link - accounts for the fact that we can't have negative OD (may be relevant for the higher dilution rates, where r is trying to be fit). 


Also worth trying to include the random effect of experiment replicate on one/all of the parameters we're trying to estimate. 

Non-linear models in `brms` require fairly informative priors. Make sure prior for theta is very loose as this is the main one we're interested in. 

The data we're trying to fit the model to are: 

```{r, fig.height=3, fig.width=4}
ggplot(chemo_dd_eqs_av, aes(y = od_blanked, 
                                      x = dilution_rate, 
                                      shape = experiment_replicate)) +
  geom_point(size = 2) + 
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)",
       title = "chemo_dd_eqs_av") +
  scale_shape_manual(values = c(15:19), name = "Replicate") +
  theme_minimal()
```
$\theta$ will describe the curvature of the line. $r$ describes the intercept with the x axis (maximum possible growth rate). $K$ describes the intercept with the y axis (carrying capacity). Based on this prior knowledge, we can provide some semi-informative priors for the data. 

We'll say r will fall somewhere between 0.9 and 1.2 

We'll say K will fall somewhere between 0.1 and 0.25

We'll give loose priors for theta, as this is the main parameter we're interest in and don't want to potentially bias the results. So we'll say it falls somewhere between -0.5 (highly sublinear) and 5 (highly supralinear).

```{r, include = F}

model <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(od_blanked ~ K * (1 - (dilution_rate / r)) ^ (1 / theta),
     K ~ 1, # + (1|experiment_replicate),
     r ~ 1, #+ (1|experiment_replicate),
     theta ~ 1, # + (1|experiment_replicate),
     nl = TRUE
  ),
  
  # data to use
  data = chemo_dd_eqs_av,
  
  # errors are gamma with identity link function (OD can't be negative)
  family = Gamma(link = "identity"), 
  
  # priors 
  prior = c(
    prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
    prior(uniform(0.10, 0.25), nlpar = "K", lb = 0.10, ub = 0.25), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5)  
  ),
  
  # hyperparameters 
  iter = 3000, warmup = 1500, chains = 4,
  control = list(adapt_delta = 0.9,
                 max_treedepth = 10),
  
)
```

```{r}
summary(model)
plot(model)
conditional_effects(model)
pairs(model)
pp_check(model, type = "scatter_avg")
```
With a normal distribution we got a huge amount of divergence and strong boundary effects for the r. In the equation, r is never able to be lower than the maximum of `dilution_rate`, the model becomes undefined (being raised to a fractional power). In theory this is makes sense - r is the maximum possible growth rate, so it should be larger than all observed growth rates. However, there is measurement error associated with `dilution_rate` so that could cause an observed value to be above the best fitting r. 

Using a gamma error distribution improves the divergence issues substantially and gives the posterior distribution of r a much more normal distribution (although bounding issue is still there).

Random effects are really difficult to fit. Even just for one parameter (r), the models don't converge well. I guess this is unsurprising given that the different experiment replicates had different ranges they tested dilution rates over. So we can get very weird estimates for the intercepts when we have no/few values towards those ends of the curves.

The theta-logistic model just doesn't seem to be flexible enough to fit the data extremely well. However, it is clear that theta is significantly above 1 - so we have concave density-dependence, or supralinear growth. 


```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Theta-logistic with user-defined functions
K <- 0.18  
r <- 1.05 
theta <- 2  
dilution_rate_seq <- seq(0, 1.1, by = 0.001)
avg_od_blanked_curve <- K * (1 - (dilution_rate_seq / r)) ^ (1 / theta)
curve_data <- data.frame(dilution_rate = dilution_rate_seq, avg_od_blanked = avg_od_blanked_curve)

# Create the plot with the original data points and model predictions
ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  geom_point() +
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  # user defined model
  geom_line(data = curve_data, aes(y = avg_od_blanked, x = dilution_rate), 
              color = "blue", size = 1) +
   
  theme_minimal()
```
Black line is the model with the gamma errors. Blue line is a model with parameters chosen by eye to just thinka bout whether the r bounding is causing fitting problems. Seems to be. If r was able to be lower, then theta would probably be higher. 

Flip the axes (for density-dependence perspective) and fit a theta-logistic model with parameters from above. 

```{r, fig.width=3, fig.height=3}

# Theta-logistic model with parameters from above
K <- 0.19   
r <- 1.09   
theta <- 1.75  
od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- r * (1 - (od_seq / K)^ theta)
curve_data <- data.frame(od_blanked = od_seq, 
                         dilution_rate = dilution_curve)
curve_data <- curve_data %>% filter(dilution_rate > 0)


# Density-dependent plot
ggplot(chemo_dd_eqs_av, aes(x = od_blanked, y = dilution_rate)) +
  geom_point() +
  labs(x = "OD", y = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # theta-logistic
  geom_line(data = curve_data, aes(x = od_blanked, y = dilution_rate), 
              color = "blue", size = 1) +
  
  theme_minimal()
```

**Problem when $\frac{g}{r} > 1$**

What is causing this affect?

If $r = g$, then 0 will be raised to the power of $1/\theta$ which will just give 0, so I can see why the model would struggle there. 

If $r < g$, then a negative value will be raised to the power of $1/\theta$ which is problematic (issues with complex numbers). 

In theory it shouldn't be possible for us to get r values that are smaller than g. However, our model is presumably not following theta-logistic perfectly and there is also error associated with `dilution_rate`. 

There are several potential options here if we are really keen to stick with the theta-logistic model: 

1. try to reparameterize the model so that we don't have this issue .

2. try to incorporate measurement error into the estimate of `dilution_rate`.

#### finding a better parameterization for r

I've tried using exp(log_r), exp(r), inv_v, inv_theta - none of these seemed to make a difference.

I also tried a logarithmic transformation: 

$$
\ln (N)=\ln (K)+\frac{1}{\theta} \ln \left(1-\frac{g}{r}\right)
$$

```{r, include = F}

model_trans <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(
    log(od_blanked) ~ log(K) + (1 / theta) * log( 1 - (dilution_rate / r)),
    #avg_od_blanked ~ K * (1 - (dilution_rate / r)) ^ (1 / theta),
    #avg_od_blanked ~ K * (1 - (((dilution_rate + e)) / r)) ^ (1 / theta),
    #avg_od_blanked ~ K * (1 - (dilution_rate / r)) ^ invtheta,
    #avg_od_blanked ~ K * (1 - (dilution_rate / r)) ^ exp(loginvtheta),
    K ~ 1,  
    r ~ 1,
    theta ~ 1,
    #invtheta ~ 1,
    #e ~ 1,
    #loginvtheta ~ 1,
    nl = TRUE  
  ),
  
  # data to use
  data = chemo_dd_eqs_av,
  
  # errors are gaussian
  family = gaussian(), 
  
  # priors 
  prior = c(
    prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
    prior(uniform(0.1, 0.25), nlpar = "K", lb = 0.1, ub = 0.25), 
    prior(uniform(0.5, 5), nlpar = "theta", lb = 0.5, ub = 5)
    #prior(uniform(log(0.2), log(10)), nlpar = "loginvtheta", lb = log(0.2), ub = log(10))
    #prior(uniform(-1, 1), nlpar = "e", lb = -1, ub = 1)  
  ),
  
  # hyperparameters 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.97,
                 max_treedepth = 10),
  
)
```


```{r}
summary(model_trans)
plot(model_trans)
conditional_effects(model_trans)
pairs(model_trans)
pp_check(model_trans, type = "scatter_avg")
```

```{r}
conditional_effects_data <- conditional_effects(model_trans)

predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

ggplot(chemo_dd_eqs_av, aes(y = log(od_blanked), x = dilution_rate)) +
  geom_point() +
  
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  theme_minimal()
```

This has the same issue as before (undefined when r is smaller than g). However, I guess because of the transformation the fit of this model is a lot better, there are basically no divergences, but r is still bounded - with a reasonably tight prior for r I get slightly better results, but interpreting all of this seems odd. Does theta still mean the density dependence that we thought? =

Also - should the error distribution be changed to Gamma as well? Probably doesn't need to be?


#### Measurement error

I had the idea of adding an "error" term to dilution rate - this would allow r to be smaller than g (if e was negative enough) - really not sure if this makes any sense.

$$
N=K\left(1-\frac{g + e}{r}\right)^{\frac{1}{\theta}}
$$
After chatting with Andrew, it turns out that there is a `brms` function called `mi()` that can be used to do this probably. Need to investigate that!

https://discourse.mc-stan.org/t/measurement-error-in-nonlinear-mixed-effect-models/17610/5

`mi()`/`me()` don't work directly with non-linear functions. You have to set up latent variables for them. Based on the link above (question answered by package author) if you do `X ~ 1 + me(dilution_rate, sd)` that would also include an intercept, which he thinks you don't need? He suggested doing: `X ~ 0 + me(dilution_rate, sd)`. 

I've tried both approaches, but in any case the fit doesn't really work. 

```{r, include = F}
model_me <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(od_blanked ~ K * (1 - (X / r)) ^ (1 / theta),
     X ~ 0 + me(dilution_rate, 0.005),
     K ~ 1, # + (1|experiment_replicate),
     r ~ 1, #+ (1|experiment_replicate),
     theta ~ 1, # + (1|experiment_replicate),
     nl = TRUE
  ),
  
  # data to use
  data = chemo_dd_eqs_av,
  
  # errors are gamma with identity link function (OD can't be negative)
  family = Gamma(link = "identity"), 
  
  # priors 
  prior = c(
    prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
    prior(uniform(0.10, 0.25), nlpar = "K", lb = 0.10, ub = 0.25), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5)  
  ),
  
  # hyperparameters 
  iter = 3000, warmup = 1500, chains = 4,
  control = list(adapt_delta = 0.9,
                 max_treedepth = 10),
  
)
```

```{r}
summary(model_me)
plot(model_me)
conditional_effects(model_me)
pairs(model_me)
#pp_check(model_me, type = "scatter_avg")
```


```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(model_me)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)


# Create the plot with the original data points and model predictions
ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  geom_point() +
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 

   
  theme_minimal()
```

Measurement error does slightly decrease the value of r and increase the value of theta in our data. 

To be honest, I think going with one of these models (probably the first one with Gamma) would be totally fine. If anything we are underestimating theta due to the issue with r and g in the flipped model. Our point is very well supported. And we can show concave down shape with other models too (see below).

The value of using a canonical model in density dependence work seems worth it to me? To discuss as a group. 


### Polynomial models 

Run a linear model, quadratic model, and quadratic model (can think about including random effects later if required)

Still need to use gamma error distribution - make sure link function is identity (default is log). Here we can use very relaxed priors as these are linear models, which `brms` is very good at fitting. 


```{r, include=FALSE}

########## linear model ##########
linear_model <- brm(
  formula = od_blanked ~ dilution_rate,
  data = chemo_dd_eqs_av,
  family = Gamma(link = "identity"), 
  priors <- c(
    set_prior("normal(0, 10)", class = "Intercept"),           
    set_prior("normal(0, 5)", class = "b", coef = "dilution_rate")   
    ), 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10)
  )

########## quadratic model ##########
quad_model <- brm(
  formula = od_blanked ~ dilution_rate + I(dilution_rate^2), 
  data = chemo_dd_eqs_av,
  family = Gamma(link = "identity"), 
  priors <- c(
    set_prior("normal(0, 10)", class = "Intercept"),           
    set_prior("normal(0, 5)", class = "b", coef = "dilution_rate"),   
    set_prior("normal(0, 5)", class = "b", coef = "Idilution_rateE2")
    ), 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10)
  )

########## cubic model ##########
cubic_model <- brm(
  formula = od_blanked ~ dilution_rate + I(dilution_rate^2) + I(dilution_rate^3) , 
  data = chemo_dd_eqs_av,
  family = Gamma(link = "identity"), 
  priors <- c(
    set_prior("normal(0, 10)", class = "Intercept"),           
    set_prior("normal(0, 5)", class = "b", coef = "dilution_rate"),   
    set_prior("normal(0, 5)", class = "b", coef = "Idilution_rateE2"),
    set_prior("normal(0, 5)", class = "b", coef = "Idilution_rateE3")
    ), 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10)
  )
```

```{r}
pp_check(linear_model, type = "scatter_avg")
pp_check(quad_model, type = "scatter_avg")
pp_check(cubic_model, type = "scatter_avg")
```


```{r}
loo(linear_model, quad_model, cubic_model)
```

Seems like quadratic is best. Cubic has very similar fit, so no benefit of including it. 

```{r}
summary(quad_model)
plot(quad_model)
conditional_effects(quad_model)
```

Intercept term controls the height of the curve (where the turning point is on the y axis)

Linear term (dilution_rate) controls the position of the curve (shifting to the centre of mass of the curve around the place)

The quadratic term (Idilution_rateE2) controls the width of the curve

```{r}
conditional_effects_data <- conditional_effects(quad_model)

predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)


# Theta-logistic with parameters from above
K <- 0.19  
r <- 1.09 
theta <- 1.75  
dilution_rate_seq <- seq(0, 1.1, by = 0.001)
avg_od_blanked_curve <- K * (1 - (dilution_rate_seq / r)) ^ (1 / theta)
curve_data <- data.frame(dilution_rate = dilution_rate_seq, avg_od_blanked = avg_od_blanked_curve)


ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  geom_point() +
  
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)") +
  
  # flipped theta logistic with parameters from Gamma model
  geom_line(data = curve_data, aes(y = avg_od_blanked, x = dilution_rate), 
              color = "blue", size = 1) +
  
  theme_minimal()
```

^ You can clearly see that a polynomial is much more flexible than the flipped theta-logistic. 



### Power model

We can also try a power model. We are looking for a simple power model that is very flexible but that is decreasing and that can be linear, concave, or convex. A simple one that we played around with in desmos was:

$$ y = ax^\frac{1}{\beta} + c $$
Where $y$ is equal to OD, $x$ is equal to dilution rate, $a$ is related to where the curve cross the $x$ axis (but not quite intercept as it varies depending on $\beta$ - it must be negative for the function to be decreasing), $\beta$ is the curvature (linear when 1, convex when greater than 1, and concave when lower than 1, must be above 0), and $c$ is the y intercept (when x is 0). 

First, I want to just create a plot that illustrates how this power model could fit all sorts of shapes for density dependence, from sublinear, to linear, to supralinear. So I want to keep the x and y axis fixed and change the curvature just as an illustration. To keep the y axis fixed is easy - keep c fixed. The x axis is more tricky.. 

Determine $a$ dynamically based on $beta$ so that the x intercept remains constant. To do this we fix $x_{int}$, the x intercept, and we fix $c$, the y intercept, then we solve for $a$ based on the value of $beta$ we have:

$$ a = \frac{-c}{x_{int}^{1/\beta}}$$

Once $a$, $/beta$, and $c$ are calculate, we can invert - express in terms of x to get the classic density-dependence perspective

$$x\ =\ \left(\frac{y-c}{a}\right)^{B}$$


Show what different values of 

```{r, fig.height=4, fig.width=10}
# Parameters that stay constant 
x <- seq(0, 1, length.out = 100)   # Fixed x-axis range 
#y <- seq(0, 1, length.out = 100)   # Fixed y-axis range
x_int <- 1                         # Fixed x-axis intercept
c <- 1                             # Fixed y-axis intercept

# setting up my betas so they are symmetric and evenly spaced
start <- 1.2    # Starting value
factor <- 1.2   # Multiplication factor
n <- 8         # number of betas above 1
beta_higher <- start * factor^(0:(n - 1))   # Betas above 1
beta_lower <- 1 / beta_higher               # Symmetric betas below 1
beta_values <- sort(c(beta_lower, 1, beta_higher))

# Dynamically generating a and getting data for each beta:a combination
data <- do.call(rbind, lapply(beta_values, function(beta) {
  a <- -c / x_int^(1 / beta)  # Adjust 'a' dynamically 
  
  # power model we are fitting
  y <- a * x^(1 / beta) + c
  
  # Inverse model: solving for x in terms of y
  x_inv <- ((y - c) / a) ^ beta  
  
  # Returning both original and inverted models
  data.frame(x = x, y = y, x_inv = x_inv, beta = beta, a = a)
  
}))

# Extract unique (beta, a) pairs, in case we want to annotate later
annotations <- unique(data[c("beta", "a")])

# Plot with log-transformed color mapping

selected_betas <- c(0.25, 0.5, 1, 2, 4) 

p1 <- ggplot(data, aes(x = x, y = y, color = log10(beta), group = beta)) +
  geom_line(size = 1) +
  labs(
    x = "Per capita growth",
    y = "Density",
    color = "Beta",
    title = "Fitted power model"
  ) +
  scale_color_gradient2(
    low = "#3BDCB4",      # Color for low beta values
    mid = "gray95",     # Neutral color for log10(beta) = 0 (beta = 1)
    high = "#185266",    # Color for high beta values
    midpoint = 0,      # log10(beta = 1) = 0
    limits = c(log10(min(data$beta)), log10(max(data$beta))), # Symmetric limits
    breaks = log10(c(0.25, 0.5, 1, 2, 4)),   # Use original beta values
    labels = c(0.25, 0.5, 1, 2, 4)        # Show actual beta values in the legend
  ) +
  theme_minimal() +
  theme(
    legend.position = "right" 
  )

p2 <- ggplot(data, aes(x = y, y = x_inv, color = log10(beta), group = beta)) +
  geom_line(size = 1) +
  labs(
    y = "Per capita growth",
    x = "Density",
    color = "Beta",
    title = "Flipped power model"
  ) +
  scale_color_gradient2(
    low = "#3BDCB4",      # Color for low beta values
    mid = "gray95",     # Neutral color for log10(beta) = 0 (beta = 1)
    high = "#185266",    # Color for high beta values
    midpoint = 0,      # log10(beta = 1) = 0
    limits = c(log10(min(data$beta)), log10(max(data$beta))), # Symmetric limits
    breaks = log10(c(0.25, 0.5, 1, 2, 4)),   # Use original beta values
    labels = c(0.25, 0.5, 1, 2, 4)        # Show actual beta values in the legend
  ) +
  theme_minimal() +
  theme(
    legend.position = "right" 
  )

plot_grid(p1, p2)
```



```{r, include = F}

power_model <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(od_blanked ~ a * (dilution_rate^(1/b)) + c, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = chemo_dd_eqs_av,
  
  # Errors are gamma with identity link function
  family = Gamma(link = "identity"),
  
  # Priors
  prior = c(
    prior(uniform(-5, 5), nlpar = "a", lb = -5, ub = 5),
    prior(uniform(0, 2), nlpar = "b", lb = 0, ub = 2),
    prior(uniform(0, 2), nlpar = "c", lb = 0, ub = 2)
  ),
  
  # Hyperparameters
  iter = 3000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.9, max_treedepth = 10)
)

```

```{r}
summary(power_model)
plot(power_model)
conditional_effects(power_model)
pairs(power_model)
pp_check(power_model, type = "scatter_avg")
```

```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(power_model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Create the plot with the original data points and model predictions
ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  geom_point() +
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
   
  theme_minimal()
```

Using gamma or Gaussian errors doesn't change the general fit or parameter estimates. Gamma probably makes sense from a more general theoretical perspective.


Flip the power model so we can see what our model looks from the classic density-dependence perspective

$$x\ =\ \left(\frac{y-c}{a}\right)^{B}$$

```{r, fig.height=3, fig.width=3}

# Flipped power model with parameters estimated from above
a <- -0.13   
b <- 0.45   
c <- 0.17  
od_seq <- seq(0, 0.25, by = 0.001)
dilution_curve <- ((od_seq - c) / a)^b
curve_data <- data.frame(od_blanked = od_seq, 
                         dilution_rate = dilution_curve)
curve_data <- curve_data %>% filter(dilution_rate > 0)


# Density-dependent plot
ggplot(chemo_dd_eqs_av, aes(x = od_blanked, y = dilution_rate)) +
  geom_point() +
  labs(x = "OD", y = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # theta-logistic
  geom_line(data = curve_data, aes(x = od_blanked, y = dilution_rate), 
              color = "blue", size = 1) +
  
  theme_minimal()
```




### Flipped power model 

The power model seems like a good model for density dependence.. So, if we are fitting the inverted data, should we use the flipped power model instead? 

$$x\ =\ \left(\frac{y-c}{a}\right)^{B}$$

^ but now change x and y to change how we model things. Interesting to see how the parameter estimates will change. 


$$y\ =\ \left(\frac{x-c}{a}\right)^{B}$$

```{r, include = F}

f_power_model <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(od_blanked ~ ((dilution_rate - c)/a)^b, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = chemo_dd_eqs_av,
  
  # Errors are gamma with identity link function
  family = Gamma(link = "identity"),
  
  # Priors
  prior = c(
    prior(uniform(-50, 0), nlpar = "a", lb = -50, ub = 0),
    prior(uniform(0, 2), nlpar = "b", lb = -2, ub = 2),
    prior(uniform(0.5, 1.5), nlpar = "c", lb = 0.5, ub = 1.5)
  ),
  
  # Hyperparameters
  iter = 3000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.9, max_treedepth = 10)
)

```

```{r}
summary(f_power_model)
plot(f_power_model)
conditional_effects(f_power_model)
pairs(f_power_model)
pp_check(f_power_model, type = "scatter_avg")
```

```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(f_power_model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Create the plot with the original data points and model predictions
ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  geom_point() +
  labs(y = "OD", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
   
  theme_minimal()
```


```{r, fig.height=3, fig.width=3}

# Power model with parameters estimated from above
a <- -23.92  
b <- 0.57  
c <- 1.09  
od_seq <- seq(0, 0.25, by = 0.001)
dilution_curve <- a * od_seq^(1/b) + c
curve_data <- data.frame(od_blanked = od_seq, 
                         dilution_rate = dilution_curve)
curve_data <- curve_data %>% filter(dilution_rate > 0)


# Density-dependent plot
ggplot(chemo_dd_eqs_av, aes(x = od_blanked, y = dilution_rate)) +
  geom_point() +
  labs(x = "OD", y = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # theta-logistic
  geom_line(data = curve_data, aes(x = od_blanked, y = dilution_rate), 
              color = "blue", size = 1) +
  
  theme_minimal()
```


### Colony counts

Probably use Gamma again - not zero inflated, not count data - it is density again, it just can't be 0. 

It is also on a very different scale so need to update priors for c (y axis intercept) and may need to rescale so that the parameters are roughly same scale - to help with fitting 

```{r}
chemo_dd_eqs_av$cfu_rescale <- chemo_dd_eqs_av$cfu_ul/100000

```


```{r, include = F}

model_cc <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(cfu_rescale ~ K * (1 - (dilution_rate / r)) ^ (1 / theta),
     K ~ 1, # + (1|experiment_replicate),
     r ~ 1, #+ (1|experiment_replicate),
     theta ~ 1, # + (1|experiment_replicate),
     nl = TRUE
  ),
  
  # data to use
  data = chemo_dd_eqs_av,
  
  # errors are gamma with identity link function (OD can't be negative)
  family = Gamma(link = "identity"), 
  
  # priors 
  prior = c(
    prior(uniform(0.8, 1.2), nlpar = "r", lb = 0.8, ub = 1.2),  
    prior(uniform(4, 16), nlpar = "K", lb = 4, ub = 16), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5)  
  ),
  
  # hyperparameters 
  iter = 3000, warmup = 1500, chains = 4,
  control = list(adapt_delta = 0.9,
                 max_treedepth = 10),
  
)
```


```{r}
summary(model_cc)
plot(model_cc)
conditional_effects(model_cc)
pairs(model_cc)
pp_check(model_cc, type = "scatter_avg")
```



```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(model_cc)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Create the plot with the original data points and model predictions
ggplot(chemo_dd_eqs_av, aes(y = cfu_rescale, x = dilution_rate)) +
  geom_point() +
  labs(y = "CFU", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
   
  theme_minimal()
```









And power model: 

```{r, include = F}

power_model_count <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(cfu_rescale ~ a * (dilution_rate^(1/b)) + c, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = chemo_dd_eqs_av,
  
  # Errors are poisson
  #family = poisson,
  family = Gamma(link = "identity"),
  
  # Priors
  prior = c(
    prior(uniform(-10, 2), nlpar = "a", lb = -10, ub = 2),
    prior(uniform(0, 2), nlpar = "b", lb = 0, ub = 2),
    prior(uniform(0, 15), nlpar = "c", lb = 0, ub = 15)
  ),
  
  # Hyperparameters
  iter = 3000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.9, max_treedepth = 10)
)

```

```{r}
summary(power_model_count)
plot(power_model_count)
conditional_effects(power_model_count)
pairs(power_model_count)
pp_check(power_model_count, type = "scatter_avg")
```

```{r}
# Extract predictions from the model
conditional_effects_data <- conditional_effects(power_model_count)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

# Create the plot with the original data points and model predictions
ggplot(chemo_dd_eqs_av, aes(y = cfu_rescale, x = dilution_rate)) +
  geom_point() +
  labs(y = "CFU", x = "Dilution rate (ml/hour/ml)", 
       color = "experiment") +
  
  # brms model predictions
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = dilution_rate)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = dilution_rate), 
              alpha = 0.2, inherit.aes = FALSE) + 
   
  theme_minimal()
```





### Theta-logistic on flipped axes

I should probably disregard this as regression is asymmetric, in the sense that it really matters what is the response and what is the predictor in terms of how variance is explained. It is assumed that the predicts are known with certainty but that there is uncertainty (residuals) around the response variable. Our experiment has OD as the response variable and dilution_rate as the predictor variable - so we have to model them that way around. No point looking at the proper theta-logistic model. 

```{r, include = F}

model2 <- brm(
  
  # model formula (parameters to fit are identified and nl flag is set)
  bf(
    dilution_rate ~ r * (1 - (avg_od_blanked / K)^ theta),
    K ~ 1,  
    r ~ 1,
    theta ~ 1, 
    nl = TRUE  
  ),
  
  # data to use
  data = equilibrium_samples,
  
  # errors are gaussian
  family = gaussian(), 
  
  # priors 
  prior = c(
    prior(uniform(0.8, 1.3), nlpar = "r", lb = 0.8, ub = 1.3),  
    prior(uniform(0.15, 0.25), nlpar = "K", lb = 0.15, ub = 0.25), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5)  
  ),
  
  # hyperparameters 
  iter = 4000, warmup = 2000, chains = 4,
  control = list(adapt_delta = 0.95,
                 max_treedepth = 10),
  
  # inital values to help with start up
  #init = function() list(K = 0.20, r = 1.1)
)
```

```{r}
summary(model2)
plot(model2)
conditional_effects(model2)
pairs(model2)
pp_check(model2, ndraws = 20)
```


```{r}
# Generate conditional effects from the model
conditional_effects_data <- conditional_effects(model2)

# Extract the data for the relevant variable, 
predicted_data <- as.data.frame(conditional_effects_data$avg_od_blanked)

# Theta-logistic model with user-defined parameters
K <- 0.18   
r <- 1.03   
theta <- 2.41  
od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- r * (1 - (od_seq / K)^ theta)
curve_data <- data.frame(avg_od_blanked = od_seq, 
                         dilution_rate = dilution_curve)
curve_data <- curve_data %>% filter(dilution_rate > 0)

# Create the plot with the original data points and model predictions
ggplot(equilibrium_samples, aes(x = avg_od_blanked, y = dilution_rate)) +
  geom_point() +
  labs(x = "OD", y = "Dilution rate (ml/hour/ml)", 
      color = "experiment") +
  
  # model predictions  
  geom_line(data = predicted_data, aes(y = estimate__, 
                                       x = avg_od_blanked)) +  
  geom_ribbon(data = predicted_data, aes(ymin = lower__, 
                                         ymax = upper__, 
                                         x = avg_od_blanked), 
              alpha = 0.2, inherit.aes = FALSE) + 
  
  # user-defined theta-logistic
  geom_line(data = curve_data, aes(x = avg_od_blanked, y = dilution_rate), 
              color = "blue", size = 1) +
    
  theme_minimal()
```

There are no divergence issues. The actual modelling fitting / parameterization all seems good, but it just seems that theta-logistic isn't the ideal fit for the data. Not quite flexible enough. The Posterior Predictive Checks don't look great. Strong pattern in the residules?

max OD is: 0.18980, posteriors for K are able to cross this - so that doesn't seem to be an issue here. 

Don't know how much we should trust this as here we are assuming there is no error in OD and that there is error in dilution_rate - this is the opposite to what we think. So I don't think fitting these models this way really makes sense. 





### Three options to model our data

The end goal is to study to shape of density-dependence. So let's start by showing the candidate models from the density dependence perspective. 

The three models are: 

- Theta logistic 

$$
g=r\left(1-\left(\frac{N}{K}\right)^\theta\right)
$$

- Power model 

$$ 
y = ax^\frac{1}{\beta} + c 
$$


- Inverted power model 

$$
x\ =\ \left(\frac{y-c}{a}\right)^{B}
$$

For each of these models, I'd like to fix the x and y intercepts and then vary the curvature of the model from sublinear, through linear, to supralinear - to give an indication of what parameters in the models we are interested in estimated in in the data.  


```{r, fig.height=4, fig.width=15}
# Parameters that stay constant 
x <- seq(0, 1, length.out = 100)    # Fixed x-axis range 
x_int <- 1                          # Fixed x-axis intercept
y_int <- 1                          # Fixed y-axis intercept

# setting up my betas so they are symmetric and evenly spaced
# theta same as beta so can use that 
start <- 1.2    # Starting value
factor <- 1.2   # Multiplication factor
n <- 8         # number of betas above 1
beta_higher <- start * factor^(0:(n - 1))   # Betas above 1
beta_lower <- 1 / beta_higher               # Symmetric betas below 1
beta_values <- sort(c(beta_lower, 1, beta_higher))

# Dynamically generating a and getting data for each beta:a combination
power_data <- do.call(rbind, lapply(beta_values, function(beta) {
  a <- -y_int / x_int^(1 / beta)  # Adjust 'a' dynamically 
  
  # power model we are fitting
  y_p <- a * x^(1 / beta) + y_int
  
  # Inverse model: solving for x in terms of y
  x_inv <- ((y_p - y_int) / a) ^ beta  
  
  # Theta logistic 
  y_t <- y_int * (1 - (x / x_int)^beta)
  
  # Returning both original and inverted models
  data.frame(x = x, y_p = y_p, x_inv = x_inv, beta = beta, y_t = y_t)
  
}))




# Plot with log-transformed color mapping

p1 <- ggplot(power_data, aes(x = x, y = y_t, color = log10(beta), group = beta)) +
  geom_line(size = 1) +
  labs(
    y = "Per capita growth",
    x = "Density",
    color = "Theta",
    title = "1. Theta logistic"
  ) +
  scale_color_gradient2(
    low = "#3BDCB4",      # Color for low beta values
    mid = "gray95",     # Neutral color for log10(beta) = 0 (beta = 1)
    high = "#185266",    # Color for high beta values
    midpoint = 0,      # log10(beta = 1) = 0
    limits = c(log10(min(data$beta)), log10(max(data$beta))), # Symmetric limits
    breaks = log10(c(0.25, 0.5, 1, 2, 4)),   # Use original beta values
    labels = c(0.25, 0.5, 1, 2, 4)        # Show actual beta values in the legend
  ) +
  theme_minimal() +
  theme(
    legend.position = "right" 
  )


p2 <- ggplot(power_data, aes(x = x, y = y_p, color = log10(beta), group = beta)) +
  geom_line(size = 1) +
  labs(
    y = "Per capita growth",
    x = "Density",
    color = "Beta",
    title = "2. Power model"
  ) +
  scale_color_gradient2(
    low = "#3BDCB4",      # Color for low beta values
    mid = "gray95",     # Neutral color for log10(beta) = 0 (beta = 1)
    high = "#185266",    # Color for high beta values
    midpoint = 0,      # log10(beta = 1) = 0
    limits = c(log10(min(data$beta)), log10(max(data$beta))), # Symmetric limits
    breaks = log10(c(0.25, 0.5, 1, 2, 4)),   # Use original beta values
    labels = c(0.25, 0.5, 1, 2, 4)        # Show actual beta values in the legend
  ) +
  theme_minimal() +
  theme(
    legend.position = "right" 
  )



p3 <- ggplot(power_data, aes(x = y_p, y = x_inv, color = log10(beta), group = beta)) +
  geom_line(size = 1) +
  labs(
    y = "Per capita growth",
    x = "Density",
    color = "Beta",
    title = "3. Inverted power model"
  ) +
  scale_color_gradient2(
    low = "#3BDCB4",      # Color for low beta values
    mid = "gray95",     # Neutral color for log10(beta) = 0 (beta = 1)
    high = "#185266",    # Color for high beta values
    midpoint = 0,      # log10(beta = 1) = 0
    limits = c(log10(min(data$beta)), log10(max(data$beta))), # Symmetric limits
    breaks = log10(c(0.25, 0.5, 1, 2, 4)),   # Use original beta values
    labels = c(0.25, 0.5, 1, 2, 4)        # Show actual beta values in the legend
  ) +
  theme_minimal() +
  theme(
    legend.position = "right" 
  )

plot_grid(p1, p2, p3, nrow = 1)
```

As our experiment takes the alternative perspective, where per capita growth rate (dilution rate) is the explantory variable. We will fit our data with the inverse versions of each of these models. 


#### 1. Flipped theta logistic 

```{r, include = F}
ft_model <- brm(
  bf(od_blanked ~ K * (1 - (dilution_rate / r)) ^ (1 / theta),
     K ~ 1, 
     r ~ 1, 
     theta ~ 1, 
     nl = TRUE
     ),
  
  data = chemo_dd_eqs_av,
  
  family = Gamma(link = "identity"), 
  
  prior = c(
    prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
    prior(uniform(0.10, 0.25), nlpar = "K", lb = 0.10, ub = 0.25), 
    prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5) 
    ),
  
  iter = 3000, warmup = 1500, chains = 4,
  control = list(adapt_delta = 0.9,
                 max_treedepth = 10),
  )


```

```{r}
summary(ft_model)
plot(ft_model)
conditional_effects(ft_model)
pairs(ft_model)
pp_check(ft_model, type = "scatter_avg")
```

```{r, fig.height=4, fig.height=12}

################### model predictions ############################
conditional_effects_data <- conditional_effects(ft_model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

p1 <- ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  labs(
    y = "Optical Density", x = "Dilution rate (ml/hour/ml)") +

  geom_ribbon(
    data = predicted_data,
    aes(ymin = lower__, ymax = upper__, x = dilution_rate),
    alpha = 0.4, inherit.aes = FALSE, fill = "#3BDCB4", color = NA
  ) +
  
  geom_line(data = predicted_data, 
            aes(y = estimate__, x = dilution_rate), 
            color = "#185266", size = 3) +
  
  geom_point(colour = "#185266", size = 5) +
  
  theme_light(base_size = 25) 



################### theta estimate ############################

posterior <- posterior_samples(ft_model)
theta_samples <- posterior$`b_theta_Intercept`
theta_mean <- posterior_summary(ft_model)["b_theta_Intercept", 1]
theta_low <- posterior_summary(ft_model)["b_theta_Intercept", 3]
theta_high <- posterior_summary(ft_model)["b_theta_Intercept", 4]

# Calculate density for posterior samples
density_data <- density(theta_samples)
density_df <- data.frame(
  theta = density_data$x,
  density = density_data$y) 

theta_density_at_mean <- approx(density_data$x, density_data$y, xout = theta_mean)$y

p2 <- ggplot(density_df, aes(x = theta, y = density)) +
  geom_line(color = "#185266", size = 2) +
  geom_area(
    data = density_df %>%
      filter(theta >= theta_low & theta <= theta_high),
    aes(x = theta, y = density),
    fill = "#3BDCB4", alpha = 0.4) +
  geom_segment(
    aes(x = theta_mean, xend = theta_mean, 
        y = 0, yend = theta_density_at_mean),
    color = "#185266", size = 2) +
  labs(
    x = expression(theta),
    y = "Frequency"
  ) +
  geom_vline(xintercept = 1, color = "gray80", size = 2, linetype = 2) +
  scale_x_log10(
    limits = c(0.25, 4),                # Set range of x-axis
    breaks = c(0.25, 0.5, 1, 2, 4)     # Logarithmic breaks
  ) +
  theme_light(base_size = 25) 


################### density dependence ############################
K <- mean(posterior$`b_K_Intercept`)
r <- mean(posterior$`b_r_Intercept`)
theta <- mean(posterior$`b_theta_Intercept`)

od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- r * (1 - (od_seq / K)^theta)
curve_data <- data.frame(od_blanked = od_seq, dilution_rate = dilution_curve) %>%
  filter(dilution_rate > 0)

p3 <- ggplot(chemo_dd_eqs_av, aes(x = od_blanked, y = dilution_rate)) +
  labs(
    x = "Density",
    y = "Per capita growth"
  ) +
  geom_line(
    data = curve_data,
    aes(x = od_blanked, y = dilution_rate),
    color = "#185266", size = 3) +
  geom_point(colour = "black", size = 5, alpha = 0.15) + 
  theme_light(base_size = 25)  # Increased base text size




# Combine the plots
right_column <- plot_grid(p2, p3, ncol = 1, rel_heights = c(1, 1), 
                          labels = c('B', 'C'),
                          label_size = 24,
                          scale = 0.95)
plot_grid(
  p1, right_column,
  nrow = 1,
  rel_widths = c(2, 1),
  labels = c('A', ''), 
  label_size = 24,
  scale = 0.95
)




```




#### 2. Flipped power function

```{r, include = F}

f_power_model <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(od_blanked ~ ((dilution_rate - c)/a)^b, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = chemo_dd_eqs_av,
  
  # Errors are gamma with identity link function
  family = Gamma(link = "identity"),
  #family = gaussian(),
  
  # Priors
  prior = c(
    prior(uniform(-80, 20), nlpar = "a", lb = -80, ub = 20),
    prior(uniform(0, 4), nlpar = "b", lb = 0, ub = 4),
    prior(uniform(0.5, 1.5), nlpar = "c", lb = 0.5, ub = 1.5)
  ),
  
  # Hyperparameters
  iter = 6000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.95, max_treedepth = 10)
)

```

```{r}
summary(f_power_model)
plot(f_power_model)
conditional_effects(f_power_model)
pairs(f_power_model)
pp_check(f_power_model, type = "scatter_avg")
```

```{r, fig.height=4, fig.height=12}

################### model predictions ############################
conditional_effects_data <- conditional_effects(f_power_model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

p1 <- ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  labs(
    y = "Optical Density", x = "Dilution rate (ml/hour/ml)") +

  geom_ribbon(
    data = predicted_data,
    aes(ymin = lower__, ymax = upper__, x = dilution_rate),
    alpha = 0.4, inherit.aes = FALSE, fill = "#3BDCB4", color = NA
  ) +
  
  geom_line(data = predicted_data, 
            aes(y = estimate__, x = dilution_rate), 
            color = "#185266", size = 3) +
  
  geom_point(colour = "#185266", size = 5) +
  
  theme_light(base_size = 25) 



################### theta estimate ############################
posterior <- posterior_samples(f_power_model)
theta_samples <- posterior$`b_b_Intercept`
theta_mean <- posterior_summary(f_power_model)["b_b_Intercept", 1]
theta_low <- posterior_summary(f_power_model)["b_b_Intercept", 3]
theta_high <- posterior_summary(f_power_model)["b_b_Intercept", 4]

# Calculate density for posterior samples
density_data <- density(theta_samples)
density_df <- data.frame(
  theta = density_data$x,
  density = density_data$y) 

theta_density_at_mean <- approx(density_data$x, density_data$y, xout = theta_mean)$y


p2 <- ggplot(density_df, aes(x = theta, y = density)) +
  geom_line(color = "#185266", size = 2) +
  geom_area(
    data = density_df %>%
      filter(theta >= theta_low & theta <= theta_high),
    aes(x = theta, y = density),
    fill = "#3BDCB4", alpha = 0.4) +
  geom_segment(
    aes(x = theta_mean, xend = theta_mean, 
        y = 0, yend = theta_density_at_mean),
    color = "#185266", size = 2) +
  labs(
    x = expression(beta),
    y = "Frequency"
  ) +
  geom_vline(xintercept = 1, color = "gray80", size = 2, linetype = 2) +
  scale_x_log10(
    limits = c(0.25, 4),                # Set range of x-axis
    breaks = c(0.25, 0.5, 1, 2, 4)     # Logarithmic breaks
  ) +
  theme_light(base_size = 25) 


################### density dependence ############################
a <- mean(posterior$`b_a_Intercept`)
c <- mean(posterior$`b_c_Intercept`)
b <- mean(posterior$`b_b_Intercept`)

od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- a * od_seq^(1 / b) + c

curve_data <- data.frame(od_blanked = od_seq, dilution_rate = dilution_curve) %>%
  filter(dilution_rate > 0)

p3 <- ggplot(chemo_dd_eqs_av, aes(x = od_blanked, y = dilution_rate)) +
  labs(
    x = "Density",
    y = "Per capita growth"
  ) +
  geom_line(
    data = curve_data,
    aes(x = od_blanked, y = dilution_rate),
    color = "#185266", size = 3) +
  geom_point(colour = "black", size = 5, alpha = 0.15) + 
  theme_light(base_size = 25)  # Increased base text size




# Combine the plots
right_column <- plot_grid(p2, p3, ncol = 1, rel_heights = c(1, 1), 
                          labels = c('B', 'C'),
                          label_size = 24,
                          scale = 0.95)
plot_grid(
  p1, right_column,
  nrow = 1,
  rel_widths = c(2, 1),
  labels = c('A', ''), 
  label_size = 24,
  scale = 0.95
)


```

#### 3. Power function

```{r, include = F}

power_model <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(od_blanked ~ a * (dilution_rate^(1/b)) + c, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = chemo_dd_eqs_av,
  
  # Errors are gamma with identity link function
  family = Gamma(link = "identity"),
  
  # Priors
  prior = c(
    prior(uniform(-5, 5), nlpar = "a", lb = -5, ub = 5),
    prior(uniform(0, 2), nlpar = "b", lb = 0, ub = 2),
    prior(uniform(0, 2), nlpar = "c", lb = 0, ub = 2)
  ),
  
  # Hyperparameters
  iter = 3000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.9, max_treedepth = 10)
)

```

```{r}
summary(power_model)
plot(power_model)
conditional_effects(power_model)
pairs(power_model)
pp_check(power_model, type = "scatter_avg")
```

```{r, fig.height=4, fig.height=12}

################### model predictions ############################
conditional_effects_data <- conditional_effects(power_model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)

p1 <- ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate)) +
  labs(
    y = "Optical Density", x = "Dilution rate (ml/hour/ml)") +

  geom_ribbon(
    data = predicted_data,
    aes(ymin = lower__, ymax = upper__, x = dilution_rate),
    alpha = 0.4, inherit.aes = FALSE, fill = "#3BDCB4", color = NA
  ) +
  
  geom_line(data = predicted_data, 
            aes(y = estimate__, x = dilution_rate), 
            color = "#185266", size = 3) +
  
  geom_line(
    data = theory_predictions,
    aes(y = final_N, x = dil),
    color = "#3BDCB4", size = 3) +
  
    geom_line(
    data = theory_predictions2,
    aes(y = final_N, x = dil),
    color = "#3BDCB4", size = 3, linetype = "dashed") +
  
  geom_point(colour = "#185266", size = 5) +
  
  theme_light(base_size = 25) 



################### theta estimate ############################
posterior <- posterior_samples(power_model)
theta_samples <- posterior$`b_b_Intercept`
theta_mean <- posterior_summary(power_model)["b_b_Intercept", 1]
theta_low <- posterior_summary(power_model)["b_b_Intercept", 3]
theta_high <- posterior_summary(power_model)["b_b_Intercept", 4]

# Calculate density for posterior samples
density_data <- density(theta_samples)
density_df <- data.frame(
  theta = density_data$x,
  density = density_data$y) 

theta_density_at_mean <- approx(density_data$x, density_data$y, xout = theta_mean)$y

p2 <- ggplot(density_df, aes(x = theta, y = density)) +
  geom_line(color = "#185266", size = 2) +
  geom_area(
    data = density_df %>%
      filter(theta >= theta_low & theta <= theta_high),
    aes(x = theta, y = density),
    fill = "#3BDCB4", alpha = 0.4) +
  geom_segment(
    aes(x = theta_mean, xend = theta_mean, 
        y = 0, yend = theta_density_at_mean),
    color = "#185266", size = 2) +
  labs(
    x = expression(beta),
    y = "Frequency"
  ) +
  geom_vline(xintercept = 1, color = "gray80", size = 2, linetype = 2) +
  scale_x_log10(
    limits = c(0.15, 6.66),                # Set range of x-axis
    breaks = c(0.2, 0.5, 1, 2, 5)     # Logarithmic breaks
  ) +
  theme_light(base_size = 25) 


################### density dependence ############################
a <- mean(posterior$`b_a_Intercept`)
c <- mean(posterior$`b_c_Intercept`)
b <- mean(posterior$`b_b_Intercept`)

od_seq <- seq(0, 0.2, by = 0.001)
dilution_curve <- ((od_seq - c) / a) ^ b

curve_data <- data.frame(od_blanked = od_seq, dilution_rate = dilution_curve) %>%
  filter(dilution_rate > 0)

p3 <- ggplot(chemo_dd_eqs_av, aes(x = od_blanked, y = dilution_rate)) +
  labs(
    x = "Density",
    y = "Per capita growth"
  ) +
  
  geom_line(
    data = curve_data,
    aes(x = od_blanked, y = dilution_rate),
    color = "#185266", size = 3) +
    
  geom_line(
    data = theory_predictions,
    aes(x = final_N, y = dil),
    color = "#3BDCB4", size = 3) +
  
  geom_point(colour = "black", size = 5, alpha = 0.15) + 
  theme_light(base_size = 25)  # Increased base text size




# Combine the plots
right_column <- plot_grid(p2, p3, ncol = 1, rel_heights = c(1, 1), 
                          labels = c('B', 'C'),
                          label_size = 24,
                          scale = 0.95)
plot_grid(
  p1, right_column,
  nrow = 1,
  rel_widths = c(2, 1),
  labels = c('A', ''), 
  label_size = 24,
  scale = 0.95
)


```

```{r}
# for andrew 

#pdf(width = 18, # The width of the plot in inches
#    height = 9) # The height of the plot in inches

#plot_grid(p3, p1, nrow = 1, labels = c('A', 'B'), scale = 0.95,
#          label_size = 24)

#dev.off()
```



### Fit power model to every subset of data

Create function that runs models and makes plots

```{r}
# Function to analyze dataset and save results as objects
analyze_dataset <- function(dataset_name) {
  # Load dataset dynamically
  dataset <- get(dataset_name)
  
  # Run the model
  power_model <- brm(
    bf(od_blanked ~ a * (dilution_rate^(1/b)) + c, 
       a ~ 1, b ~ 1, c ~ 1, nl = TRUE),
    data = dataset,
    family = Gamma(link = "identity"),
    prior = c(
      prior(uniform(-5, 5), nlpar = "a", lb = -5, ub = 5),
      prior(uniform(0, 2), nlpar = "b", lb = 0, ub = 2),
      prior(uniform(0, 2), nlpar = "c", lb = 0, ub = 2)
    ),
    iter = 3000, warmup = 1500, chains = 4,
    control = list(adapt_delta = 0.95, max_treedepth = 12)
  )
  
  # Save summary as an object
  model_summary <- summary(power_model)
  
  # Generate pp_check plot
  pp_plot <- pp_check(power_model, type = "scatter_avg")
  
  # Generate conditional effects plot
  conditional_effects_data <- conditional_effects(power_model)
  predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)
  
  p1 <- ggplot(dataset, aes(y = od_blanked, x = dilution_rate)) +
    labs(
      y = "", x = "") +
  
    geom_ribbon(
      data = predicted_data,
      aes(ymin = lower__, ymax = upper__, x = dilution_rate),
      alpha = 0.4, inherit.aes = FALSE, fill = "#3BDCB4", color = NA
    ) +
    
    geom_line(data = predicted_data, 
              aes(y = estimate__, x = dilution_rate), 
              color = "#185266", size = 3) +
    
    geom_point(colour = "#185266", size = 5) +
    
    theme_light(base_size = 25) 
  
  ################### theta estimate ############################
  posterior <- posterior_samples(power_model)
  theta_samples <- posterior$`b_b_Intercept`
  theta_mean <- posterior_summary(power_model)["b_b_Intercept", 1]
  theta_low <- posterior_summary(power_model)["b_b_Intercept", 3]
  theta_high <- posterior_summary(power_model)["b_b_Intercept", 4]
  
  # Calculate density for posterior samples
  density_data <- density(theta_samples)
  density_df <- data.frame(
    theta = density_data$x,
    density = density_data$y) 
  
  theta_density_at_mean <- approx(density_data$x, density_data$y, xout = theta_mean)$y
  
  p2 <- ggplot(density_df, aes(x = theta, y = density)) +
    geom_line(color = "#185266", size = 2) +
    geom_area(
      data = density_df %>%
        filter(theta >= theta_low & theta <= theta_high),
      aes(x = theta, y = density),
      fill = "#3BDCB4", alpha = 0.4) +
    geom_segment(
      aes(x = theta_mean, xend = theta_mean, 
          y = 0, yend = theta_density_at_mean),
      color = "#185266", size = 2) +
    labs(
      x = "",
      y = ""
    ) +
    geom_vline(xintercept = 1, color = "gray80", size = 2, linetype = 2) +
    scale_x_log10(
      limits = c(0.15, 6.66),                # Set range of x-axis
      breaks = c(0.2, 0.5, 1, 2, 5)          # Logarithmic breaks
    ) +
    theme_light(base_size = 25) 
  
    
  # Return all results as a list
  return(list(
    summary = model_summary,
    pp_check_plot = pp_plot,
    fit = p1,
    posteriors = p2
  ))
}

```

Run the function for each dataset and store everything in a list

```{r, include=FALSE}
# List of dataset names
datasets <- c("chemo_dd_eqs_av", "chemo_dd_eqs_last", "chemo_dd_eqs_first", 
              "chemo_dd_eqs_day1", "chemo_dd_eqs_day2", "chemo_dd_eqs_day3", 
              "chemo_dd_eqs_day4")

# Run the function for all datasets and save results in a named list
results <- lapply(datasets, function(name) {
  analyze_dataset(name)
})
names(results) <- datasets

# Access example outputs:
# - Model summary for `chemo_dd_eqs_av`: results[[1]]$summary

```


```{r, fig.height=12, fig.width=36}

# Extract "fit" and "posteriors" plots for each dataset
fit_plots <- lapply(results, function(res) res$fit)
posteriors_plots <- lapply(results, function(res) res$posteriors)


# Create plot grids for the top (fit) and bottom (posteriors) rows
fit_row <- plot_grid(plotlist = fit_plots, ncol = 7, scale = 0.9,
                     labels = datasets, label_size = 16, label_x = 0.15)
posteriors_row <- plot_grid(plotlist = posteriors_plots, ncol = 7, scale = 0.9)

# Combine rows with titles
plot_grid(
  fit_row, posteriors_row, 
  ncol = 1
)
```

### Flipped theta logistic to every subset of data

```{r}
# Function to analyze dataset and save results as objects
analyze_dataset2 <- function(dataset_name) {
  # Load dataset dynamically
  dataset <- get(dataset_name)
  
  # Run the model
  ft_model <- brm(
    bf(od_blanked ~ K * (1 - (dilution_rate / r)) ^ (1 / theta),
       K ~ 1, 
       r ~ 1, 
       theta ~ 1, 
       nl = TRUE
       ),
    data = dataset,
    family = Gamma(link = "identity"), 
    prior = c(
      prior(uniform(0.9, 1.2), nlpar = "r", lb = 0.9, ub = 1.2),  
      prior(uniform(0.10, 0.25), nlpar = "K", lb = 0.10, ub = 0.25), 
      prior(uniform(-0.5, 5), nlpar = "theta", lb = -0.5, ub = 5) 
      ),
    iter = 3000, warmup = 1500, chains = 4,
    control = list(adapt_delta = 0.9,
                   max_treedepth = 10),
    )

  
  # Save summary as an object
  model_summary <- summary(ft_model)
  
  # Generate pp_check plot
  pp_plot <- pp_check(ft_model, type = "scatter_avg")
  
  # Generate conditional effects plot
  conditional_effects_data <- conditional_effects(ft_model)
  predicted_data <- as.data.frame(conditional_effects_data$dilution_rate)
  
  p1 <- ggplot(dataset, aes(y = od_blanked, x = dilution_rate)) +
    labs(
      y = "", x = "") +
  
    geom_ribbon(
      data = predicted_data,
      aes(ymin = lower__, ymax = upper__, x = dilution_rate),
      alpha = 0.4, inherit.aes = FALSE, fill = "#3BDCB4", color = NA
    ) +
    
    geom_line(data = predicted_data, 
              aes(y = estimate__, x = dilution_rate), 
              color = "#185266", size = 3) +
    
    geom_point(colour = "#185266", size = 5) +
    
    theme_light(base_size = 25) 
  
  ################### theta estimate ############################
  posterior <- posterior_samples(ft_model)
  theta_samples <- posterior$`b_theta_Intercept`
  theta_mean <- posterior_summary(ft_model)["b_theta_Intercept", 1]
  theta_low <- posterior_summary(ft_model)["b_theta_Intercept", 3]
  theta_high <- posterior_summary(ft_model)["b_theta_Intercept", 4]
  
  # Calculate density for posterior samples
  density_data <- density(theta_samples)
  density_df <- data.frame(
    theta = density_data$x,
    density = density_data$y) 
  
  theta_density_at_mean <- approx(density_data$x, density_data$y, xout = theta_mean)$y
  
  p2 <- ggplot(density_df, aes(x = theta, y = density)) +
    geom_line(color = "#185266", size = 2) +
    geom_area(
      data = density_df %>%
        filter(theta >= theta_low & theta <= theta_high),
      aes(x = theta, y = density),
      fill = "#3BDCB4", alpha = 0.4) +
    geom_segment(
      aes(x = theta_mean, xend = theta_mean, 
          y = 0, yend = theta_density_at_mean),
      color = "#185266", size = 2) +
    labs(
      x = expression(theta),
      y = "Frequency"
    ) +
    geom_vline(xintercept = 1, color = "gray80", size = 2, linetype = 2) +
    scale_x_log10(
      limits = c(0.25, 4),                # Set range of x-axis
      breaks = c(0.25, 0.5, 1, 2, 4)     # Logarithmic breaks
    ) +
    theme_light(base_size = 25) 
  
    
  # Return all results as a list
  return(list(
    summary = model_summary,
    pp_check_plot = pp_plot,
    fit = p1,
    posteriors = p2
  ))
}

```

Run the function for each dataset and store everything in a list

```{r, include=FALSE}
# List of dataset names
datasets <- c("chemo_dd_eqs_av", "chemo_dd_eqs_last", "chemo_dd_eqs_first", 
              "chemo_dd_eqs_day1", "chemo_dd_eqs_day2", "chemo_dd_eqs_day3", 
              "chemo_dd_eqs_day4")

# Run the function for all datasets and save results in a named list
results <- lapply(datasets, function(name) {
  analyze_dataset2(name)
})
names(results) <- datasets

# Access example outputs:
# - Model summary for `chemo_dd_eqs_av`: results[[1]]$summary

```


```{r, fig.height=12, fig.width=36}

# Extract "fit" and "posteriors" plots for each dataset
fit_plots <- lapply(results, function(res) res$fit)
posteriors_plots <- lapply(results, function(res) res$posteriors)


# Create plot grids for the top (fit) and bottom (posteriors) rows
fit_row <- plot_grid(plotlist = fit_plots, ncol = 7, scale = 0.9,
                     labels = datasets, label_size = 16, label_x = 0.15)
posteriors_row <- plot_grid(plotlist = posteriors_plots, ncol = 7, scale = 0.9)

# Combine rows with titles
plot_grid(
  fit_row, posteriors_row, 
  ncol = 1
)
```


<br>
<br>
<br>

### Dilution rates calculated with max volume 

```{r, include = F}

power_model <- brm(
  # Model formula (parameters to fit are identified and nl flag is set)
  bf(od_blanked ~ a * (dilution_rate_2^(1/b)) + c, 
    a ~ 1,  
    b ~ 1,  
    c ~ 1,  
    nl = TRUE
  ),
  # Data to use
  data = chemo_dd_eqs_av,
  
  # Errors are gamma with identity link function
  family = Gamma(link = "identity"),
  
  # Priors
  prior = c(
    prior(uniform(-5, 5), nlpar = "a", lb = -5, ub = 5),
    prior(uniform(0, 2), nlpar = "b", lb = 0, ub = 2),
    prior(uniform(0, 2), nlpar = "c", lb = 0, ub = 2)
  ),
  
  # Hyperparameters
  iter = 3000, 
  warmup = 1500, 
  chains = 4,
  control = list(adapt_delta = 0.9, max_treedepth = 10)
)

```

```{r}
summary(power_model)
plot(power_model)
conditional_effects(power_model)
pairs(power_model)
pp_check(power_model, type = "scatter_avg")
```

```{r, fig.height=4, fig.height=12}

################### model predictions ############################
conditional_effects_data <- conditional_effects(power_model)
predicted_data <- as.data.frame(conditional_effects_data$dilution_rate_2)

p1 <- ggplot(chemo_dd_eqs_av, aes(y = od_blanked, x = dilution_rate_2)) +
  labs(
    y = "Optical Density", x = "Dilution rate (ml/hour/ml)") +

  geom_ribbon(
    data = predicted_data,
    aes(ymin = lower__, ymax = upper__, x = dilution_rate_2),
    alpha = 0.4, inherit.aes = FALSE, fill = "#3BDCB4", color = NA
  ) +
  
  geom_line(data = predicted_data, 
            aes(y = estimate__, x = dilution_rate_2), 
            color = "#185266", size = 3) +
  
  geom_line(
    data = theory_predictions,
    aes(y = final_N, x = dil),
    color = "#3BDCB4", size = 3) +
  
  
  geom_point(colour = "#185266", size = 5) +
  
  theme_light(base_size = 25) 



################### theta estimate ############################
posterior <- posterior_samples(power_model)
theta_samples <- posterior$`b_b_Intercept`
theta_mean <- posterior_summary(power_model)["b_b_Intercept", 1]
theta_low <- posterior_summary(power_model)["b_b_Intercept", 3]
theta_high <- posterior_summary(power_model)["b_b_Intercept", 4]

# Calculate density for posterior samples
density_data <- density(theta_samples)
density_df <- data.frame(
  theta = density_data$x,
  density = density_data$y) 

theta_density_at_mean <- approx(density_data$x, density_data$y, xout = theta_mean)$y

p2 <- ggplot(density_df, aes(x = theta, y = density)) +
  geom_line(color = "#185266", size = 2) +
  geom_area(
    data = density_df %>%
      filter(theta >= theta_low & theta <= theta_high),
    aes(x = theta, y = density),
    fill = "#3BDCB4", alpha = 0.4) +
  geom_segment(
    aes(x = theta_mean, xend = theta_mean, 
        y = 0, yend = theta_density_at_mean),
    color = "#185266", size = 2) +
  labs(
    x = expression(beta),
    y = "Frequency"
  ) +
  geom_vline(xintercept = 1, color = "gray80", size = 2, linetype = 2) +
  scale_x_log10(
    limits = c(0.15, 6.66),                # Set range of x-axis
    breaks = c(0.2, 0.5, 1, 2, 5)     # Logarithmic breaks
  ) +
  theme_light(base_size = 25) 


################### density dependence ############################
a <- mean(posterior$`b_a_Intercept`)
c <- mean(posterior$`b_c_Intercept`)
b <- mean(posterior$`b_b_Intercept`)

od_seq <- seq(0, 0.2, by = 0.00001)
dilution_curve <- ((od_seq - c) / a) ^ b

curve_data <- data.frame(od_blanked = od_seq, dilution_rate_2 = dilution_curve) %>%
  filter(dilution_rate_2 > 0)

p3 <- ggplot(chemo_dd_eqs_av, aes(x = od_blanked, y = dilution_rate_2)) +
  labs(
    x = "Density",
    y = "Per capita growth"
  ) +
  geom_line(
    data = curve_data,
    aes(x = od_blanked, y = dilution_rate_2),
    color = "#185266", size = 3) +
  
  geom_line(
    data = theory_predictions,
    aes(x = final_N, y = dil),
    color = "#3BDCB4", size = 3) +
  
  geom_point(colour = "black", size = 5, alpha = 0.15) + 
  theme_light(base_size = 25)  # Increased base text size




# Combine the plots
right_column <- plot_grid(p2, p3, ncol = 1, rel_heights = c(1, 1), 
                          labels = c('B', 'C'),
                          label_size = 24,
                          scale = 0.95)
plot_grid(
  p1, right_column,
  nrow = 1,
  rel_widths = c(2, 1),
  labels = c('A', ''), 
  label_size = 24,
  scale = 0.95
)


```

<br>
<br>




