---
title: "chemo-dd"
subtitle: "Data prep"
output: 
  html_notebook:
    toc: true
    toc_depth: 4
author: "James Orr"
---

# Introduction

## Summary

This R notebook is used for organising the data (colony counts, optical density, outflow volumes, chibio optical density timeseries) from the density-dependence experiments with *E. coli* in chemostats in the Letten lab. 

dd1-dd4 were conducted by Kaleigh Davis but dd1 failed and produced no data. dd5-dd7 were conducted by James Orr and Alicia Williams but dd5 failed and produced no data. Experiments that are used in this study are: dd2, dd3, dd4, dd6, and dd7 

## Set up environment 

Load packages and clear environment 

```{r}
#### Required packages
library(tidyverse)  
library(readr)
library(stringr)
library(lubridate)

#### Clear  environment 
rm(list = ls())   
```

Load data

```{r}
# cfu data
cfu_jo <- read.csv("data/cfu/cfu_counts_jo.csv")
cfu_kd <- read.csv("data/cfu/cfu_counts_kd.csv")

# od data
od_jo <- read.csv("data/od/od_jo.csv")
od_kd <- read.csv("data/od/od_kd.csv")

# outflow data
outflow_jo <- read.csv("data/outflow/outflow_rates_jo.csv")
outflow_kd <- read.csv("data/outflow/outflow_rates_kd.csv")
```

# Prepare each data type

## Colony counts

Merge data and do some initial filtering 

```{r}
# join two dataframes
cfu <- cfu_jo %>%
  select(-inflow_setting)  %>%
  bind_rows(cfu_kd) %>%
  select(-date)

# filtering out data that we can't use
cfu <- cfu %>%
  # remove the samples that were taken after inflow pumps in dd6 
  filter(time != "a") %>%
  select(-c(time)) %>%
  # remove overnights
  filter(day != "0") %>%
  # remove media tests
  filter(sample_media != "media") %>%
  select(-c(sample_media)) %>%
  # keep only lb plates
  filter(plate_type == "lb") %>%
  select(-c(plate_type))

# remove old dataframes
rm(list = c("cfu_jo", "cfu_kd"))   

```

Average counts

```{r}
# mean counts per dot (1 to 10) with NAs removed 
cfu <- cfu %>%
  mutate(av_counts = rowMeans(across(dot1:dot10), na.rm = TRUE))
```

Convert counts to densities (i.e., CFU per ul)

```{r}
cfu <- cfu %>%
  
  # multiply by 10^dilution factor to get CFUs per ul 
  # remember that the very first row is already diluted by 10
  # first row is 100ul of sample so it would be: count * 10 ^ 1 to get 1ul
  # sixth row would be: count * 10 ^ 6 to get to 1ul
  mutate(cfu_undil = av_counts * 10^dilution_factor) %>%
  
  # each dot is 5ul, so divide by 5 to get counts per ul
  mutate(cfu_ul = cfu_undil/5)
```

Final cfu dataframe

```{r}
cfu <- cfu %>%
  select(c(experiment_replicate, day, ID, cfu_ul))

# there were issue with the CFU data from day 3 of dd6 - serial dilution went wrong for two data points, which were an order of magnitude higher than all other data. No issues with OD data, which is used for the analysis. 

cfu <- cfu %>%
  mutate(cfu_ul = if_else(experiment_replicate == "dd6" & 
                          ID %in% c("M2", "M4") & 
                          day == 3, 
                          NaN, cfu_ul))
```

## Outflow data

Merge dataframes

```{r}
outflow <- outflow_jo %>%
  # make the names the same between dataframes 
  rename(volume_out_round = volume_out) %>%
  rename(time_elapsed_round = time_elapsed) %>%
  # merge dataframes
  bind_rows(outflow_kd) %>%
  # remove variables we don't need 
  select(-c(volume_out_full, time_elapsed_full, 
            exp_start_date, day_measured, species,
            media.bottle)) %>%
  # remove dd1 (failed after 10 hours)
  filter(experiment_rep != "dd1")

# remove old dataframes
rm(list = c("outflow_jo", "outflow_kd"))   
```

Get volume per hour 

```{r}
# for each observation, divide total volume by total number of hours
outflow <- outflow %>%
  mutate(outflow_per_hour = volume_out_round/time_elapsed_round) 
```

Average across each measurement of the same reactor - in some experiments the outflow from some reactors was measured multiple times. So we can just use the average outflow_per_hour to represent the outflow across the entire experiment. 

```{r}
outflow <- outflow %>%
  group_by(bioreactor, experiment_rep, inflow_setting) %>%
  summarise(outflow_per_hour_mean = mean(outflow_per_hour))

# outflow volume correlates with inflow settings very well
#plot(outflow$inflow_setting, outflow$outflow_per_hour_mean)
```

Calculate dilution rate and final cleaning of outflow data. Dilution rate is volume of media supplied per hour divided by the volume of the culture (standard approach in chemostat work).  

The volume of the culture for most of a cycle is 21ml. However, between inflow and outflow there is a larger volume of liquid in the reactor. The bigger the dilution rate the greater this value is. Therefore we need to calculate the time-average volume in the chemostat. 

First step will be to estimate the average time between inflow and outflow pumps. We need the Chi.Bio log files for this and we only have these files for dd7. So across all pumps we'll take the average of the time between inflow and outflow and use this single average value for all experiments and all reactors. It is a good approximation as experimental procedures were consistent. 

```{r}
log_file <- readLines("data/chibio/dd7/dd7-log.txt")
log_file <- log_file[-(1:5)] # Drop the first 5 lines (general info)

# convert to a tidy data frame
log_df <- tibble(raw = log_file) %>%
  mutate(
    # Try to match a date and time at the start using regex 
    # first match is a data ([0-9]{4}-[0-9]{2}-[0-9]{2})
    # then one or more spaces []+
    # second match is a time ([0-9:.+-]+)
    # then one or more spaces []+
    # third match is everything that is left
    match = str_match(raw, 
                      "^([0-9]{4}-[0-9]{2}-[0-9]{2})[ ]+([0-9:.+-]+)[ ]+(.*)"),
    
    # extract the matches into columns (first match is the full line)
    full_line = match[,1],
    date = match[,2],
    time = match[,3],
    message = match[,4]
  ) %>%
  select(full_line, date, time, message)

# select lines related to the pumps 
log_df <- log_df %>%
  filter(str_starts(message, "Pump"))

# create a datetime variable
log_df <- log_df %>%
  mutate(datetime = ymd_hms(paste(date, time), truncated = 3))

# remove the first 5 hours and last 5 hours (set up and shut down of experiment)
# define start and end cutoffs and filter the dataframe
start_cutoff <- min(log_df$datetime, na.rm = TRUE) + hours(5)
end_cutoff <- max(log_df$datetime, na.rm = TRUE) - hours(5)
log_df <- log_df %>%
  filter(datetime > start_cutoff, datetime < end_cutoff)

# there are matching numbers of inflows and outflows, starting at M0 inflow
# ending on outflow M7 - so we have 4208/2/8 = 263 20 minute cycles, perfect! 

# create variable for reactor and inflow or outflow
log_df <- log_df %>%
  mutate(reactor = str_extract(message, "M[0-9]+")) %>%
  # regex to extract time in front of "ms" 
  mutate(pump_time = as.numeric(str_extract(message, 
                                               "\\d+(\\.\\d+)?(?=ms)"))) %>%
  mutate(event_type = if_else(pump_time < 12000, "inflow", "outflow"))


# calculate the time between inflow and outflow pumps
# separate the inflow and outflow events 
inflow_pumps <- log_df %>% 
  filter(event_type == "inflow") %>%
  select(event_type, reactor, datetime)
outflow_pumps <- log_df %>% filter(event_type == "outflow") %>%
  select(event_type, reactor, datetime)

# just add in - they are at a 1:1 match based on rows already 
time_difference <- inflow_pumps %>%
  mutate(datetime2 = outflow_pumps$datetime) %>%
  mutate(time_diff = as.numeric(difftime(datetime2, datetime, 
                                         units = "secs"))) %>%
  select(-event_type)

# calculate average time difference overall 
time_difference_average <- mean(time_difference$time_diff, na.rm = TRUE)

# calculate average time per reactor 
time_difference_reactors <- time_difference %>%
  group_by(reactor) %>%
  summarise(mean_time_diff = mean(time_diff, na.rm = TRUE))

# clean environment
rm(log_df, inflow_pumps, outflow_pumps, end_cutoff, start_cutoff, log_file)
```

On average the time between inflow and outflow is 141.85 seconds or **11.8%** of a cycle. No significant changes over time (about two seconds over four days) and no big differences between reactors (range of 129 to 153 seconds). So we can be confident that using this average time is a good approximation for all experiments and reactors.

Next we need to calculate the volumes associated with an inflow setting. I can use the outflow volumes to work this out. Because inflow pumps run every 20 minutes, they run 72 times a day. Therefore daily outflow volume divided by 72 will be the volume of water that comes into the chemostats at each inflow cycle. Adding this volume to 21 will give the max liquid volume of the chemostats in a given cycle. 

```{r}
proportion_at_max <- time_difference_average / (20*60)
proportion_at_min <- 1 - proportion_at_max

outflow <- outflow %>%
  
  # inflow volume is hourly outflow volume divided by number of inflow cycles (3)
  mutate(inflow_volume = outflow_per_hour_mean/3) %>%
  
  # maximum volume of reactor is 21ml + inflow volume 
  mutate(max_volume = inflow_volume + 21) %>%
  
  # time average volume 
  mutate(time_average_volume = (21*proportion_at_min) + (max_volume*proportion_at_max)) %>%
  
  # dilution rate (outflow volume per hour / time average volume)
  mutate(dilution_rate = outflow_per_hour_mean/time_average_volume) %>%
  
  # option to just use minimum volume 
  #mutate(dilution_rate = outflow_per_hour_mean/21) %>%

  # rename some variables to match cfu and od
  rename(experiment_replicate = experiment_rep) %>%
  rename(ID = bioreactor)
           
rm(proportion_at_max, proportion_at_min, 
   time_difference_average, time_difference_reactors, time_difference)        
```



## OD data

Merge dataframes and quick clean up

```{r}
od <- od_jo %>%
  bind_rows(od_kd)

# remove the sample that was taken after inflow pumps in dd6 (time == "a")
od <- od %>%
  filter(time != "a") %>%
  select(-c(time))

# remove old dataframes
rm(list = c("od_jo", "od_kd"))   
```

Blanking across each combination of experiment replicate and day

```{r}
# group the dataframe by the unique combination of experiment_replicate and day
od <- od %>%
  group_by(experiment_replicate, day) %>%

  # create a new variable 'OD_control_value' that stores the OD value where OD_control == 1
  mutate(OD_control_value = ifelse(OD_control == 1, OD, NA)) %>%

  # fill missing values (NA) within each group by carrying the OD_control_value down and up
  fill(OD_control_value, .direction = "downup") %>%
  
  # create new variable for blanked od
  mutate(od_blanked = OD - OD_control_value) %>%
  
  # set negative values to 0 (when controls were blanked with media - tiny differences)
  mutate(od_blanked = ifelse(od_blanked < 0, 0, od_blanked))
```

Some final cleaning 

```{r}
od <- od %>%
  
  # keep the reactor values only 
  filter(ID %in% c("M0", "M1", "M2", "M3",
                   "M4", "M5", "M6", "M7")) %>%
  
  # select the variables we want
  select(c(experiment_replicate, day, ID, od_blanked))
  

```


## Chibio timeseries

This is just for illustrative purposes. Use dd7 as this was only experiments where the system wasn't restarted at some stage (so ODs are uninterrupted and not reblanked) and where the full spectrum of dilution rates were tested. 

Load and merge the data

```{r, warning=FALSE}

# Define the file path (for dd7)
file_path <- "data/chibio/dd7/2024-09-24/"

# Get a list of all CSV files in the directory
csv_files <- list.files(path = file_path, pattern = "*.csv", full.names = TRUE)

# Loop through each file and read it in, naming the dataframe MX
for (file in csv_files) {
  # Extract the MX part from the file name (e.g., M0, M1, etc.)
  file_name <- str_extract(basename(file), "M[0-9]+")
  
  # Read the file (show_col_types = FALSE to repress a warning message)
  data <- read_csv(file, show_col_types = FALSE)
  
  # Add a column for the reactor
  data$ID <- file_name
  
  # Assign the data to an object named after the MX part of the file name
  assign(file_name, data)
}

# Merge M0 to M7 into one dataframe called chibio
chibio <- bind_rows(M0, M1, M2, M3, M4, M5, M6, M7)

# Clean up the environment
rm(list = c("M0", "M1", "M2", "M3",
            "M4", "M5", "M6", "M7",
            "data", "csv_files", "file",
            "file_name", "file_path"))   

```

Clean up the chibio data a little 

```{r}
outflow_dd7 <- outflow %>%
  filter(experiment_replicate == "dd7")

chibio <- chibio %>%
  select(exp_time, od_measured, ID) %>%
  left_join(outflow_dd7)

chibio$hours <- chibio$exp_time / 3600
```


# Merge the data

Merge the cfu, od, and outflow data. 

```{r}
# merge all three dataframes
chemo_dd <- cfu %>%
  left_join(od) %>%
  left_join(outflow)

# create a factor for the day by experiment interaction
chemo_dd$sample <- interaction(chemo_dd$experiment_replicate, chemo_dd$day)

```

Do some initial cleaning and filtering

```{r}

# remove controls and reactors that failed 
chemo_dd <- chemo_dd %>%
  
  # for dd2 none are removed (control was innoculated)
  
  # for dd3, remove control (M4)
  filter(!(ID %in% c("M4") & experiment_replicate == "dd3")) %>%
  
  # for dd4, remove control (M6)
  filter(!(ID %in% c("M6") & experiment_replicate == "dd4")) %>%

  # for dd6, remove control (M5) and M1, whose pump stopped during day 1
  filter(!(ID %in% c("M1", "M5") & experiment_replicate == "dd6")) %>%
  
  # for dd7, remove control (M2) and remove M3 from day 3 onwards, pump failed at end of day 2
  filter(!(ID %in% c("M2") & experiment_replicate == "dd7")) %>%
  filter(!(ID %in% c("M3") & experiment_replicate == "dd7" & day %in% c(3, 4)))

```

# Select equilibrium samples

As we are only interested in samples taken while populations are at equilibrium, we need to think carefully about population dynamics and about the potential impacts of evolution. We know that populations in the low dilution rates can overshoot the equilibrium and then come back down after a few days, and we know that populations in the highest dilution rates take much longer to reach equilibrium.   

Furthermore, evolution could potentially increase equilibrium densities, which would make our estimates of density dependence less accurate. Evolution could be especially important at high dilution rates (strongest selection pressure but lower population sizes) and intermediate dilution rates (still a relatively strong selection pressure and high population sizes). Supporting this concern is the fact that biofilms were observed on the walls of the chemostats in some intermediate and high dilution rates at the end of dd4 (seen by KD and AL) and dd6 (seen by JO, AW and AL).  

## Heuristics for sample selection

From looking at the OD time series, and from considering the impacts of evolution, we can define some **general heuristics** for when populations reached equilibrium and when impacts of evolution are likely to be minimal: 

- For low dilution rates the population overshoots and doesn't come back to equilibrium until day four. This has also been seen in previous chemostat experiments at this dilution rate in the Letten lab. Beyond day four, evolution could influence results. **We therefore only consider day four for low dilution rates.**

- For intermediate dilution rates, equilibrium was reached within one day. These populations have a strong selection pressure and high population sizes, so evolution is a risk at later sampling points. **We therefore only consider days one and two relevant for intermediate dilution rates.**

- For high dilution rates, OD is either not detectable or it gradually increases until around day three or four. After day four, the risk of evolution is too great - this is when biofilms started forming in dd4 and dd6. **We therefore only consider days 3 and 4 for high dilution rates.**

Choosing cutoffs that separate low, intermediate, and high dilutions requires some thought. 

The low to intermediate cutoff is pretty easy - all samples with dilution rate of 0.061 and below include samples from the lowest inflow settings where overshooting happens. The next inflow setting gives dilution rates all above 0.1 and there are no more overshooting dynamics. So choosing 0.1 is fine for that cut-off.  

The intermediate to high cut-off is a bit more difficult to choose. Looking at the OD per day plots for each replicate (especially dd4 and dd7), there is gradual increases in *all* dilutions above 0.75 and the inflow settings corresponding to these dilution rates lead to slow population growths that take several days to reach equilibrium. Below 0.75 and down to 0.25 there are some populations that increased each day and some that decreased each day, and some that stayed the same. These variable results could be related to noise or evolution. 

Based on these observations (illustrated in `2-exploratory-plots.RMD`), I'll go with 0.1 and 0.7 as the two cutoffs. Our results are not sensitive to these choices. 

Varying these cutoffs or ignoring the heuristics and just choosing the final day of the experiments doesn't qualtitatively change the result we find (of superlinear density dependence). Fred Smith did a similar experiment in 1963 with Daphnia and an expanding culture approach rather than a harvest rate approach - he had similar issues deciding when populations were at equilibrium. When talking about the samples he removed he said: "Perhaps more should be omitted in some cases, but more elegant screening methods would seem to enhance the possibility of bias." We've tried to strike a balance between mechanistic understanding and amount of screening to avoid this possibility of bias as well.

## Different filtering procedures

We will create seven "final" datasets to do all analyses on to show that different filtering procedures don't qualitatively impact results. 

1. `chemo_dd_eqs_av`: keep all relevant samples that meet heuristics above and take averages across reactors from the same experiment that were sampled on multiple relevant days (always at equilibrium). 

2. `chemo_dd_eqs_last`: of all relevant samples that meet heuristics above, keep only the sample taken on the last relevant day. 

3. `chemo_dd_eqs_first`: of all relevant samples that meet heuristics above, keep only the sample taken on the first relevant day. 

4. (-7) `chemo_dd_eqs_dayX`: Ignore the heuristics above and take all samples from the same day for days 1 through 4. Day 4 makes the most sense probably if you want to take data all from the same day - enough time has passed for all samples to have had the potential to reach equlibrium. This assumes that evolution has no impact. Note that there is a lower number of samples in these datasets due to some experiment replicates not lasting until day four or not being sampled every day. 

First create a general dataset of all samples that follow the heuristics above. 

```{r}
# use 0.1 and 0.7 as low-intermediate cutoff and intermediate-high cutoff, respectively 
chemo_dd_eqs <- chemo_dd %>%
  
  # overshoot only seen in the lowest dilution: take at least day 4
  filter(!(dilution_rate < 0.1 & day %in% c(1, 2, 3))) %>%
  
  # highest dilutions need time to get to equilibrium: take at least day 3 
  filter(!(dilution_rate > 0.7 & day %in% c(1, 2))) %>%
  
  # mid dilution rates get to equilibrium fast but could then evolve, take days 1 and 2 only
  filter(!(dilution_rate < 0.7 & dilution_rate > 0.1 & day %in% c(3, 4, 5, 6))) %>%
  
  # longer than 4 days is too much, equilibrium already reached but high risk of evolution
  filter(!(day > 4))
```

Next, create "final" datasets that follow different filtering procedures outlined above. 

```{r}
##### 1) take averages across all relevant days ##### 
# Mid dilutions, averaged across days 1 and 2
# High dilutions, averaged across days 3 and 4 
chemo_dd_eqs_av <- chemo_dd_eqs %>%
  select(-c(sample)) %>%
  group_by(experiment_replicate, ID, inflow_setting, 
           outflow_per_hour_mean, dilution_rate) %>%
  summarise(od_blanked = mean(od_blanked, na.rm = TRUE),
            cfu_ul = mean(cfu_ul, na.rm = TRUE),
            n = n(),
            days = paste(unique(day), collapse = "-"))

##### 2) keep only the sample taken on the last relevant day #####  
# slice_max will take the row with the maximum value of day
chemo_dd_eqs_last <- chemo_dd_eqs %>%
  select(-c(sample)) %>%
  group_by(experiment_replicate, ID, inflow_setting, 
           outflow_per_hour_mean, dilution_rate) %>%
  slice_max(day, with_ties = FALSE) %>%
  mutate(n = 1)

##### 3) keep only the sample taken on the first relevant day  ##### 
# slice_min will take the row with the minimum value of day
chemo_dd_eqs_first <- chemo_dd_eqs %>%
  select(-c(sample)) %>%
  group_by(experiment_replicate, ID, inflow_setting, 
           outflow_per_hour_mean, dilution_rate) %>%
  slice_min(day, with_ties = FALSE) %>%
  mutate(n = 1)

##### 4) keep samples from day 1 only  ##### 
# filtering from chemo_dd not chemo_dd_eqs as we aren't following heuristics
chemo_dd_eqs_day1 <- chemo_dd %>%
  select(-c(sample)) %>%
  filter(day == 1) %>%
  mutate(n = 1)

##### 5) keep samples from day 2 only  ##### 
# filtering from chemo_dd not chemo_dd_eqs as we aren't following heuristics
chemo_dd_eqs_day2 <- chemo_dd %>%
  select(-c(sample)) %>%
  filter(day == 2) %>%
  mutate(n = 1)

##### 6) keep samples from day 3 only  ##### 
# filtering from chemo_dd not chemo_dd_eqs as we aren't following heuristics
chemo_dd_eqs_day3 <- chemo_dd %>%
  select(-c(sample)) %>%
  filter(day == 3) %>%
  mutate(n = 1)

##### 7) keep samples from day 4 only  ##### 
# filtering from chemo_dd not chemo_dd_eqs as we aren't following heuristics
chemo_dd_eqs_day4 <- chemo_dd %>%
  select(-c(sample)) %>%
  filter(day == 4) %>%
  mutate(n = 1)

```


# Save processed data 

```{r}
write.csv(chemo_dd, "data/processed/all_samples.csv", row.names = F)
write.csv(chibio, "data/processed/chibio.csv", row.names = F)
write.csv(chemo_dd_eqs, "data/processed/equilibrium_samples.csv", row.names = F)
write.csv(chemo_dd_eqs_av, "data/processed/equilibrium_samples_av.csv", row.names = F)
write.csv(chemo_dd_eqs_last, "data/processed/equilibrium_samples_last.csv", row.names = F)
write.csv(chemo_dd_eqs_first, "data/processed/equilibrium_samples_first.csv", row.names = F)
write.csv(chemo_dd_eqs_day1, "data/processed/equilibrium_samples_day1.csv", row.names = F)
write.csv(chemo_dd_eqs_day2, "data/processed/equilibrium_samples_day2.csv", row.names = F)
write.csv(chemo_dd_eqs_day3, "data/processed/equilibrium_samples_day3.csv", row.names = F)
write.csv(chemo_dd_eqs_day4, "data/processed/equilibrium_samples_day4.csv", row.names = F)
```


